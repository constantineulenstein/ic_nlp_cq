{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task_1_main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3PxOsuS7Bi0g",
        "huUfT43sLAmc",
        "y9brja9zLFR4",
        "uYSdwJ1XLVia"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5830468a03d5475694fc35370f6223b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c36ef91e67524463acb199c5b4bef341",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_785dbccd9a844327b7b09e474c260af7",
              "IPY_MODEL_d8d7564a801d443fa8001abb3cfcc18e"
            ]
          }
        },
        "c36ef91e67524463acb199c5b4bef341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "785dbccd9a844327b7b09e474c260af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c7a8221e455d4741ae5233e7bb17c3ef",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d0debe500b847aea57fad925e59c129"
          }
        },
        "d8d7564a801d443fa8001abb3cfcc18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_610d20bcc02949538de4c7245019a7e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:04&lt;00:00, 46.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_996a8032758f4d84bfc8b1543ac01534"
          }
        },
        "c7a8221e455d4741ae5233e7bb17c3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d0debe500b847aea57fad925e59c129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "610d20bcc02949538de4c7245019a7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "996a8032758f4d84bfc8b1543ac01534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "360d1c8f8f7e4ad6ae54f471d7015274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4df7c46e658b4be78640a1678d15a408",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2b1f3fa53a54bc883eb8ac5060bbfa4",
              "IPY_MODEL_4302fdad42de41a9bedd26811876b380"
            ]
          }
        },
        "4df7c46e658b4be78640a1678d15a408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2b1f3fa53a54bc883eb8ac5060bbfa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c5496ff15fcf47cdb013195b1ad9d8b2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9abc19e9631c41d8bb47c4505dc28e80"
          }
        },
        "4302fdad42de41a9bedd26811876b380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ef0a8b23a24d452fa190550ed93dec26",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:06&lt;00:00, 68.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8d95711d1de452a9dcab4c5e890840e"
          }
        },
        "c5496ff15fcf47cdb013195b1ad9d8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9abc19e9631c41d8bb47c4505dc28e80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef0a8b23a24d452fa190550ed93dec26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8d95711d1de452a9dcab4c5e890840e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f591cefdde0c4c2996d6f52ec50de456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_635c0903f0ce4c9a8410a0c1ba2c231d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e579bc2ea49b44e79f3468371ee3893d",
              "IPY_MODEL_6798e20c60b74a74adaa110d7e96f9c0"
            ]
          }
        },
        "635c0903f0ce4c9a8410a0c1ba2c231d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e579bc2ea49b44e79f3468371ee3893d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6031453ebb454f6799115a8d55ef54c2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ebc0c48a7a94249b3e1ad65b6ae14a6"
          }
        },
        "6798e20c60b74a74adaa110d7e96f9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e79e711149bd47aa9165711e161ebee5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 73.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57c0385dad314b648e74bb50af7d24bb"
          }
        },
        "6031453ebb454f6799115a8d55ef54c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ebc0c48a7a94249b3e1ad65b6ae14a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e79e711149bd47aa9165711e161ebee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57c0385dad314b648e74bb50af7d24bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNppdFiGaRLE"
      },
      "source": [
        "### Coursework coding instructions (please also see full coursework spec)\n",
        "\n",
        "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
        "\n",
        "For the task you choose you will need to do two approaches:\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "We should be able to run both approaches from the same colab file\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "\n",
        "#### Reproducibility:\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "Good luck! We are really looking forward to seeing your reports and your model code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8IanU_MaRLH"
      },
      "source": [
        "#Todo\n",
        "#Remove punctuation\n",
        "#Use embedding that represents headslines\n",
        "#Tokenizer with special token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WfDREnZaRLI",
        "outputId": "a5ff1ea5-f7dc-4c68-ab93-4d83920726b9"
      },
      "source": [
        "# You will need to download any word embeddings required for your code, e.g.:\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
        "\n",
        "#! pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-18 08:09:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-18 08:09:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-18 08:09:38--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.01MB/s    in 6m 52s  \n",
            "\n",
            "2021-02-18 08:16:30 (2.00 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYZiUqXFghTY",
        "outputId": "c6e49dc0-b7ff-4d9b-a6c7-c7e6e4c1bfad"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6XoHzG_bHDo",
        "outputId": "963c81c7-f70a-4db7-c889-2e87fe053e4a"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=45df18c2063b1cd794f1b5d51c5d60edb35b852b5cb215370e48c2b02ac6a2a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFcC8gvDaRLI"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs\n",
        "from transformers import RobertaTokenizer, RobertaModel, BertTokenizer, BertModel, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import re\n",
        "import pdb\n",
        "import random\n",
        "import csv\n",
        "import tqdm\n",
        "import multiprocessing\n",
        "import pickle\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ09zSA9Jq3b",
        "outputId": "fdca77cd-7679-4ef6-e5da-40157e69475c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 20 15:56:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84OyAVNSaRLJ",
        "outputId": "b58f6265-216a-4c4e-e9a2-cac31d95d279"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2gxbblziaRLK",
        "outputId": "8e2ef5b5-9b5d-4bcf-dd19-a1089adc6639"
      },
      "source": [
        "# Load data\n",
        "%mkdir ./data\n",
        "%cd ./data\n",
        "import os\n",
        "\n",
        "if not os.path.isfile('train.csv'): \n",
        "  !wget -O train.csv https://www.dropbox.com/s/utcewlslgwm278m/train.csv?dl=0\n",
        "if not os.path.isfile('dev.csv'): \n",
        "  !wget -O dev.csv https://www.dropbox.com/s/0bpqmpb009ay717/dev.csv?dl=0\n",
        "    \n",
        "%cd ..\n",
        "\n",
        "train_df = pd.read_csv('./data/train.csv')\n",
        "test_df = pd.read_csv('./data/dev.csv')\n",
        "train_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n",
            "--2021-02-20 15:56:09--  https://www.dropbox.com/s/utcewlslgwm278m/train.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/utcewlslgwm278m/train.csv [following]\n",
            "--2021-02-20 15:56:09--  https://www.dropbox.com/s/raw/utcewlslgwm278m/train.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb4c7688a997c2fbd8199de3597.dl.dropboxusercontent.com/cd/0/inline/BJRCTvjL4ZIlxtO4j1papyC-LGQOpW4NiK0bF8WcRhoZZ72OCuxlW4LVmdfk6OseN6zQEYSYSs3QDMAD39oDWlZ5mlbYmxRSGr4Sa970oCXl08nPyj-7eSxUf8xiTJPKmzA/file# [following]\n",
            "--2021-02-20 15:56:12--  https://ucb4c7688a997c2fbd8199de3597.dl.dropboxusercontent.com/cd/0/inline/BJRCTvjL4ZIlxtO4j1papyC-LGQOpW4NiK0bF8WcRhoZZ72OCuxlW4LVmdfk6OseN6zQEYSYSs3QDMAD39oDWlZ5mlbYmxRSGr4Sa970oCXl08nPyj-7eSxUf8xiTJPKmzA/file\n",
            "Resolving ucb4c7688a997c2fbd8199de3597.dl.dropboxusercontent.com (ucb4c7688a997c2fbd8199de3597.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to ucb4c7688a997c2fbd8199de3597.dl.dropboxusercontent.com (ucb4c7688a997c2fbd8199de3597.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 947914 (926K) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 925.70K  1.74MB/s    in 0.5s    \n",
            "\n",
            "2021-02-20 15:56:13 (1.74 MB/s) - ‘train.csv’ saved [947914/947914]\n",
            "\n",
            "--2021-02-20 15:56:13--  https://www.dropbox.com/s/0bpqmpb009ay717/dev.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/0bpqmpb009ay717/dev.csv [following]\n",
            "--2021-02-20 15:56:13--  https://www.dropbox.com/s/raw/0bpqmpb009ay717/dev.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc98d67df7a1c848340d003697f9.dl.dropboxusercontent.com/cd/0/inline/BJREZyxWTptEsXyPC4FYzU7TIfRsr6shIig1wtA3wNGTNQ1a_HZpbrHQOm-lV2hebu2H2TWiapptwDeU7zOZrUvJJFYAyisBVDwf2YBW6VjMxbLxVbFEMEjucsEPOJ1TQ08/file# [following]\n",
            "--2021-02-20 15:56:14--  https://uc98d67df7a1c848340d003697f9.dl.dropboxusercontent.com/cd/0/inline/BJREZyxWTptEsXyPC4FYzU7TIfRsr6shIig1wtA3wNGTNQ1a_HZpbrHQOm-lV2hebu2H2TWiapptwDeU7zOZrUvJJFYAyisBVDwf2YBW6VjMxbLxVbFEMEjucsEPOJ1TQ08/file\n",
            "Resolving uc98d67df7a1c848340d003697f9.dl.dropboxusercontent.com (uc98d67df7a1c848340d003697f9.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc98d67df7a1c848340d003697f9.dl.dropboxusercontent.com (uc98d67df7a1c848340d003697f9.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 219349 (214K) [text/plain]\n",
            "Saving to: ‘dev.csv’\n",
            "\n",
            "dev.csv             100%[===================>] 214.21K   706KB/s    in 0.3s    \n",
            "\n",
            "2021-02-20 15:56:14 (706 KB/s) - ‘dev.csv’ saved [219349/219349]\n",
            "\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original</th>\n",
              "      <th>edit</th>\n",
              "      <th>grades</th>\n",
              "      <th>meanGrade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14530</td>\n",
              "      <td>France is ‘ hunting down its citizens who join...</td>\n",
              "      <td>twins</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13034</td>\n",
              "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
              "      <td>bowling</td>\n",
              "      <td>33110</td>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8731</td>\n",
              "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
              "      <td>party</td>\n",
              "      <td>22100</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>76</td>\n",
              "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
              "      <td>slap</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6164</td>\n",
              "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
              "      <td>school</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9647</th>\n",
              "      <td>10899</td>\n",
              "      <td>State officials blast ' unprecedented ' DHS &lt;m...</td>\n",
              "      <td>idea</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9648</th>\n",
              "      <td>1781</td>\n",
              "      <td>Protesters Rally for &lt;Refugees/&gt; Detained at J...</td>\n",
              "      <td>stewardesses</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9649</th>\n",
              "      <td>5628</td>\n",
              "      <td>Cruise line Carnival Corp. joins the fight aga...</td>\n",
              "      <td>raisin</td>\n",
              "      <td>21000</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9650</th>\n",
              "      <td>14483</td>\n",
              "      <td>Columbia police hunt woman seen with &lt;gun/&gt; ne...</td>\n",
              "      <td>cake</td>\n",
              "      <td>32200</td>\n",
              "      <td>1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9651</th>\n",
              "      <td>5255</td>\n",
              "      <td>Here 's What 's In The House-Approved Health &lt;...</td>\n",
              "      <td>food</td>\n",
              "      <td>11000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9652 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... meanGrade\n",
              "0     14530  ...       0.2\n",
              "1     13034  ...       1.6\n",
              "2      8731  ...       1.0\n",
              "3        76  ...       0.4\n",
              "4      6164  ...       0.0\n",
              "...     ...  ...       ...\n",
              "9647  10899  ...       0.0\n",
              "9648   1781  ...       0.4\n",
              "9649   5628  ...       0.6\n",
              "9650  14483  ...       1.4\n",
              "9651   5255  ...       0.4\n",
              "\n",
              "[9652 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMrfmRD0rPJ_"
      },
      "source": [
        "indexNames = train_df[(train_df['grades'] == 0)].index\n",
        "# Delete these row indexes from dataFrame\n",
        "train_df.drop(indexNames , inplace=True)\n",
        "train_df.reset_index(inplace=True,drop=True)\n",
        "pd.set_option('display.max_rows', 10)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "bGImqJ7ycEyh",
        "outputId": "8c6a1420-ac77-41d6-f5b1-6bbbbcc0173b"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original</th>\n",
              "      <th>edit</th>\n",
              "      <th>grades</th>\n",
              "      <th>meanGrade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14530</td>\n",
              "      <td>France is ‘ hunting down its citizens who join...</td>\n",
              "      <td>twins</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13034</td>\n",
              "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
              "      <td>bowling</td>\n",
              "      <td>33110</td>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8731</td>\n",
              "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
              "      <td>party</td>\n",
              "      <td>22100</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>76</td>\n",
              "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
              "      <td>slap</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8832</td>\n",
              "      <td>All 22 &lt;promises/&gt; Trump made in his speech to...</td>\n",
              "      <td>sounds</td>\n",
              "      <td>22200</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9124</th>\n",
              "      <td>12770</td>\n",
              "      <td>Among Republicans , Trump is more popular than...</td>\n",
              "      <td>interns</td>\n",
              "      <td>21100</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9125</th>\n",
              "      <td>1781</td>\n",
              "      <td>Protesters Rally for &lt;Refugees/&gt; Detained at J...</td>\n",
              "      <td>stewardesses</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9126</th>\n",
              "      <td>5628</td>\n",
              "      <td>Cruise line Carnival Corp. joins the fight aga...</td>\n",
              "      <td>raisin</td>\n",
              "      <td>21000</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9127</th>\n",
              "      <td>14483</td>\n",
              "      <td>Columbia police hunt woman seen with &lt;gun/&gt; ne...</td>\n",
              "      <td>cake</td>\n",
              "      <td>32200</td>\n",
              "      <td>1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9128</th>\n",
              "      <td>5255</td>\n",
              "      <td>Here 's What 's In The House-Approved Health &lt;...</td>\n",
              "      <td>food</td>\n",
              "      <td>11000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9129 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... meanGrade\n",
              "0     14530  ...       0.2\n",
              "1     13034  ...       1.6\n",
              "2      8731  ...       1.0\n",
              "3        76  ...       0.4\n",
              "4      8832  ...       1.2\n",
              "...     ...  ...       ...\n",
              "9124  12770  ...       0.8\n",
              "9125   1781  ...       0.4\n",
              "9126   5628  ...       0.6\n",
              "9127  14483  ...       1.4\n",
              "9128   5255  ...       0.4\n",
              "\n",
              "[9129 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQbm--SyaRLL"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PxOsuS7Bi0g"
      },
      "source": [
        "### Training for BILSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxhwWCz6aRLL"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch, bert=False):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    train_losses = np.zeros(number_epoch)\n",
        "    valid_losses = np.zeros(number_epoch)\n",
        "    print(\"Training model.\")\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            feature, target = batch\n",
        "            feature_1= feature[0].to(device)\n",
        "            feature_2 = feature[1].to(device)\n",
        "            target = target.to(device)\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden_1 = model.init_hidden()\n",
        "            model.hidden_2 = model.init_hidden()\n",
        "\n",
        "            feature = (feature_1, feature_2)\n",
        "            predictions = model(feature).squeeze(1)\n",
        "\n",
        "            #print(predictions.shape)\n",
        "            #print(target.shape)\n",
        "\n",
        "            loss = loss_fn(predictions, target)\n",
        "                        \n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy()) \n",
        "            \n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        train_losses[epoch-1] = epoch_loss\n",
        "        valid_losses[epoch-1] = valid_loss\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')\n",
        "    return train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAPacdgOaRLM"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "            feature_1= feature[0].to(device)\n",
        "            feature_2 = feature[1].to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            feature = (feature_1, feature_2)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden_1 = model.init_hidden()\n",
        "            model.hidden_2 = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            #predictions.requires_grad = True\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcchQKtEaRLN"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "uO4Kv0URaRLN",
        "outputId": "e814cbe4-28c4-4a51-8209-0462920a7a1e"
      },
      "source": [
        "'''\n",
        "def create_vocab(data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "\n",
        "            tokenized_sentence.append(token)\n",
        "\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-83-d3ae31c862b2>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    return vocabulary, tokenized_corpus\u001b[0m\n\u001b[0m                                       \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSQ-36SdaRLO"
      },
      "source": [
        "# To create our vocab\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    \n",
        "    replacement_re = re.compile(r'^<*/>') #do not split replacement format\n",
        "    prefix_re = re.compile(r'''^[\\[\\(\"]''')\n",
        "    suffix_re = re.compile(r''',[\\]\\)\"']$''')\n",
        "    infix_re = re.compile(r'''[-\\,.~]''')\n",
        "    \n",
        "    \n",
        "    return Tokenizer(nlp.vocab,\n",
        "                     token_match = replacement_re.match,\n",
        "                     prefix_search=prefix_re.search,\n",
        "                     suffix_search=suffix_re.search,\n",
        "                     infix_finditer = infix_re.finditer  \n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "def preprocessor(data,edits):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    punctuation = \"\\\":\\.,\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    nlp.tokenizer = custom_tokenizer(nlp)\n",
        "    \n",
        "    tokenized_corpus_1= [] # Let us put the tokenized corpus in a list\n",
        "    tokenized_corpus_2= []\n",
        "    \n",
        "    \n",
        "    for i, sentence in enumerate(data):\n",
        "\n",
        "        sentence = sentence.lower()\n",
        "        tokenized_sentence_1 = []\n",
        "        tokenized_sentence_2 = []\n",
        "\n",
        "        for token in nlp(sentence): # simplest split is\n",
        "\n",
        "            if token.text in punctuation:\n",
        "                continue\n",
        "            else:\n",
        "                \n",
        "                if token.text[0] == '<':\n",
        "                    tokenized_sentence_1.append(\"???\")\n",
        "                    tokenized_sentence_2.append(\"???\")\n",
        "                    \n",
        "                    tokenized_sentence_2.append(edits[i])\n",
        "\n",
        "                    \n",
        "                    tokenized_sentence_1.append(token.text[1:-2])\n",
        "                else:\n",
        "                    tokenized_sentence_2.append(token.text)\n",
        "                \n",
        "                    tokenized_sentence_1.append(token.text)\n",
        "\n",
        "\n",
        "        tokenized_corpus_1.append(tokenized_sentence_1)\n",
        "        tokenized_corpus_2.append(tokenized_sentence_2)\n",
        "        #print(tokenized_corpus_1[:5])\n",
        "        #print(tokenized_corpus_2[:5])\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus_1:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "    \n",
        "    for token in edits:\n",
        "        \n",
        "        if token not in vocabulary:\n",
        "            \n",
        "            vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus_1, tokenized_corpus_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "W5v3MdT0MK7c",
        "outputId": "f550260f-338d-4078-ad6d-806db1e09ac8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-0b6b6e58d64e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_corpus_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_corpus_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-fe08eb21fbfd>\u001b[0m in \u001b[0;36mpreprocessor\u001b[0;34m(data, edits)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mtokenized_sentence_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"???\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mtokenized_sentence_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_faSla0aknz"
      },
      "source": [
        "'''\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def preprocessor_bert(data,edits):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    next = False\n",
        "    punctuation = \"\\\":\\.,\"\n",
        "    #nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    #nlp.tokenizer = custom_tokenizer(nlp)\n",
        "    \n",
        "    tokenized_corpus_1= [] # Let us put the tokenized corpus in a list\n",
        "    tokenized_corpus_2= []\n",
        "    tokenized_mask_corpus = []\n",
        "    \n",
        "    \n",
        "    for i, sentence in enumerate(data):\n",
        "\n",
        "        sentence = sentence.lower()\n",
        "        tokenized_sentence_1 = []\n",
        "        tokenized_sentence_2 = []\n",
        "        tokenized_mask_sentence = []\n",
        "\n",
        "        for token in tokenizer_bert.tokenize(sentence): # simplest split is\n",
        "\n",
        "            if token in punctuation:\n",
        "                continue\n",
        "            else:\n",
        "                \n",
        "                if token == '<':\n",
        "                    print(token)\n",
        "                    next = True\n",
        "\n",
        "                    \n",
        "                    #tokenized_sentence_2.append(edits[i])\n",
        "\n",
        "                    \n",
        "                    #tokenized_sentence_1.append(token[1:-2])\n",
        "                elif token == '>' or token == '/':\n",
        "                    continue\n",
        "                else:\n",
        "                    tokenized_sentence_1.append(token)\n",
        "                    \n",
        "                    if next:\n",
        "                        tokenized_sentence_2.append(edits[i])\n",
        "                        tokenized_mask_sentence.append(1)\n",
        "                        next = False\n",
        "                    else:\n",
        "                        tokenized_sentence_2.append(token)\n",
        "                        tokenized_mask_sentence.append(0)\n",
        "                \n",
        "        tokenized_corpus_1.append(tokenized_sentence_1)\n",
        "        tokenized_corpus_2.append(tokenized_sentence_2)\n",
        "        tokenized_mask_corpus.append(tokenized_mask_sentence)\n",
        "        print(tokenized_corpus_1[:5])\n",
        "        print(tokenized_corpus_2[:5])\n",
        "        print(tokenized_mask_corpus[:5])\n",
        "        if i==3:\n",
        "          raise\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus_1:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "    \n",
        "    for token in edits:\n",
        "        \n",
        "        if token not in vocabulary:\n",
        "            \n",
        "            vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus_1, tokenized_corpus_2\n",
        "preprocessor_bert(training_data,train_df['edit'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOOe3Ca7aRLP"
      },
      "source": [
        "'''def collate_fn_padd(batch):\n",
        "    #We add padding to our minibatches and create tensors for our model\n",
        "\n",
        "\n",
        "    batch_labels = [l for f, l in batch]\n",
        "    batch_features = [f for f, l in batch]\n",
        "\n",
        "    batch_features_len = [len(f) for f, l in batch]\n",
        "\n",
        "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\n",
        "\n",
        "    return seq_tensor, batch_labels\n",
        "'''\n",
        "def collate_fn_padd(batch):\n",
        "    \n",
        "    #We add padding to our minibatches and create tensors for our model\n",
        "    \n",
        "\n",
        "    batch_labels = [l for f, g, l in batch]\n",
        "    batch_features = [(f,g) for f, g, l in batch]\n",
        "    \n",
        "\n",
        "    batch_features_len = [len(f) for f, g, l in batch]\n",
        "\n",
        "\n",
        "    seq_tensor_1 = torch.zeros((len(batch), 50)).long()\n",
        "    seq_tensor_2 = torch.zeros((len(batch), 50)).long()\n",
        "\n",
        "    \n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor_1[idx, :seqlen] = torch.LongTensor(seq[0])\n",
        "        seq_tensor_2[idx, :seqlen] = torch.LongTensor(seq[1])\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\n",
        "    \n",
        "\n",
        "    return (seq_tensor_1, seq_tensor_2), batch_labels\n",
        "\n",
        "'''\n",
        "def collate_fn_pad(batch):\n",
        "\n",
        "    original, edit, labels = zip(*batch)\n",
        "    padded_original = torch.nn.utils.rnn.pad_sequence(original, batch_first=True,padding_value=0)\n",
        "    padded_edit = torch.nn.utils.rnn.pad_sequence(edit, batch_first=True,padding_value=0)\n",
        "    labels = torch.Tensor(labels)\n",
        "    return (padded_org, padded_edit, labels)\n",
        "'''\n",
        "\n",
        "\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, train_data, labels):\n",
        "        self.x_train = train_data\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train[item], self.y_train[item]\n",
        "    \n",
        "class Task1Dataset_double(Dataset):\n",
        "\n",
        "    def __init__(self, train_data_1,train_data_2, labels):\n",
        "        self.x_train_1 = train_data_1\n",
        "        self.x_train_2 = train_data_2\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train_1[item],self.x_train_2[item], self.y_train[item]    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F1kXohWaRLR"
      },
      "source": [
        "class BiLSTM_double(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(BiLSTM_double, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm_1 = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.lstm_2 = nn.LSTM(embedding_dim,hidden_dim, bidirectional = True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.fc = nn.Linear(2*50*100, hidden_dim*2)\n",
        "        self.fc2 = nn.Linear(hidden_dim*2, hidden_dim//2)\n",
        "        self.fc3 = nn.Linear(hidden_dim//2, hidden_dim//4)\n",
        "\n",
        "        self.hidden_1 = self.init_hidden()\n",
        "        self.hidden_2 = self.init_hidden()\n",
        "        self.hidden2label = nn.Linear(hidden_dim//4, 1)\n",
        "\n",
        "\n",
        "        self.d1 = nn.Dropout(0.3)\n",
        "        self.d2 = nn.Dropout(0.3)\n",
        "        self.d3 = nn.Dropout(0.3)\n",
        "        self.d4 = nn.Dropout(0.3)\n",
        "        self.d5 = nn.Dropout(0.3)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        \n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim, requires_grad = True).to(self.device), \\\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim, requires_grad = True).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        self.embedded_1 = self.embedding(sentence[0])\n",
        "        self.embedded_1 = self.embedded_1.permute(1, 0, 2) #Needed for LSTMs\n",
        "\n",
        "        self.embedded_2 = self.embedding(sentence[1])\n",
        "        self.embedded_2 = self.embedded_2.permute(1, 0, 2) #Needed for LSTMs\n",
        "\n",
        "        #lstm_out : (seq_len,batch_size,num_directions (2) * hidden_size (50))\n",
        "        #hidden : (num_layers * num_directions, batch_size,hidden_size)\n",
        "        lstm_out_1, self.hidden_1 = self.lstm_1(\n",
        "            self.embedded_1.view(len(self.embedded_1), self.batch_size, self.embedding_dim), self.hidden_1)\n",
        "        \n",
        "        lstm_out_1 = F.leaky_relu(self.d1(lstm_out_1))\n",
        "\n",
        "        lstm_out_2, self.hidden_2 = self.lstm_2(\n",
        "            self.embedded_2.view(len(self.embedded_2), self.batch_size, self.embedding_dim), self.hidden_2)\n",
        "        \n",
        "        lstm_out_2 = F.leaky_relu(self.d2(lstm_out_2))\n",
        "        \n",
        "        #out : (1)\n",
        "        lstm_out_1 = lstm_out_1.permute(1,0,2)\n",
        "        lstm_out_2 = lstm_out_2.permute(1,0,2)\n",
        "\n",
        "        out1 = self.fc(torch.cat((lstm_out_1.reshape(self.batch_size, -1),lstm_out_2.reshape(self.batch_size, -1)), dim = 1))\n",
        "        out1 = F.leaky_relu(self.d3(out1))\n",
        "\n",
        "        out2 = self.fc2(out1)\n",
        "        out2 = F.leaky_relu(self.d4(out2))\n",
        "\n",
        "        out3 = self.fc3(out2)\n",
        "        out3 = F.leaky_relu(self.d5(out3))\n",
        "\n",
        "        out = self.hidden2label(out3)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JSWm-Q5nIxX"
      },
      "source": [
        "## Approach 1 code, using functions defined above:\n",
        "\n",
        "# We set our training data and test data\n",
        "training_data = train_df['original']\n",
        "test_data = test_df['original']\n",
        "\n",
        "# Creating word vectors\n",
        "#training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
        "#test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
        "training_vocab, training_tokenized_corpus_1,training_tokenized_corpus_2=  preprocessor(training_data,train_df['edit'])\n",
        "test_vocab, test_tokenized_corpus_1,test_tokenized_corpus_2=  preprocessor(test_data,test_df['edit'])\n",
        "\n",
        "#print(\"Vocabulary individual creation - done\")\n",
        "\n",
        "# Creating joint vocab from test and train:\n",
        "#joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\n",
        "joint_vocab, joint_tokenized_corpus_1,joint_tokenized_corpus_2 = preprocessor(pd.concat([training_data, test_data]),pd.concat([train_df['edit'],test_df['edit']],ignore_index = True))\n",
        "\n",
        "print(\"Vocabulary joined creation - done\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Vocab created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeXy7v7dnQGq"
      },
      "source": [
        "# We create representations for our tokens\n",
        "wvecs = [] # word vectors\n",
        "word2idx = [] # word2index\n",
        "idx2word = []\n",
        "\n",
        "#Add special character -> embedding vector of ones \n",
        "wvecs.append(np.ones(100))\n",
        "\n",
        "# This is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f:\n",
        "  index = 1 #zero padding\n",
        "  for line in f.readlines():\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in joint_vocab:\n",
        "          (word, vec) = (word,\n",
        "                     list(map(float,line.strip().split()[1:])))\n",
        "          wvecs.append(vec)\n",
        "          word2idx.append((word, index))\n",
        "          idx2word.append((index, word))\n",
        "          index += 1\n",
        "\n",
        "wvecs = np.array(wvecs)\n",
        "word2idx = dict(word2idx)\n",
        "idx2word = dict(idx2word)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ze3glIrVMmj"
      },
      "source": [
        "word2idx['<unk>'] = 1\n",
        "idx2word[1] = '<unk>'\n",
        "mean = np.mean(wvecs, axis=0) # initialize unknown token as mean\n",
        "#wvecs = np.vstack((wvecs, mean))\n",
        "wvecs[0] = mean\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgcGpchbQOO-"
      },
      "source": [
        "\n",
        "\n",
        "vectorized_seqs_1 = [[word2idx[tok] if tok in word2idx else word2idx['<unk>'] for tok in seq] for seq in training_tokenized_corpus_1]\n",
        "vectorized_seqs_2 = [[word2idx[tok] if tok in word2idx else word2idx['<unk>'] for tok in seq] for seq in training_tokenized_corpus_2]\n",
        "\n",
        "\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\n",
        "vectorized_seqs_1 = [x if len(x) > 0 else [0] for x in vectorized_seqs_1]\n",
        "vectorized_seqs_2 = [x if len(x) > 0 else [0] for x in vectorized_seqs_2]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XzJHcQ_aRLT",
        "scrolled": true
      },
      "source": [
        "\n",
        "INPUT_DIM = len(word2idx)\n",
        "EMBEDDING_DIM = wvecs.shape[1]\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "model = BiLSTM_double(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(params)\n",
        "#print(\"Total number of parameters is: {​​}​​\".format(params))\n",
        "\n",
        "print(model)\n",
        "\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model.to(device)\n",
        "# We provide the model with our embeddings\n",
        "#x = np.concatenate((wvecs,wvecs),axis=1)\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8ovZX23ECX4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(train_losses,valid_losses,num_epochs):\n",
        "  epochs = list(range(num_epochs))\n",
        "  plt.plot(epochs,train_losses, label='train')\n",
        "  plt.plot(epochs,valid_losses, label='valid')\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un9wo5TPyN9L"
      },
      "source": [
        "\n",
        "feature_1 = vectorized_seqs_1\n",
        "feature_2 = vectorized_seqs_2\n",
        "\n",
        "\n",
        "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
        "#train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\n",
        "train_and_dev = Task1Dataset_double(feature_1,feature_2, train_df['meanGrade'])\n",
        "\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,\n",
        "                                           (train_examples,\n",
        "                                            dev_examples))\n",
        "####Shuffle might need to be true. Check later\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "train_losses, valid_losses = train(train_loader, dev_loader, model, epochs)\n",
        "plot(train_losses, valid_losses, len(train_losses))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJHjgo5XEELW"
      },
      "source": [
        "# BERT VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huUfT43sLAmc"
      },
      "source": [
        "## Bert Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGdZjHS-INyj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YhnKBjYLIoG"
      },
      "source": [
        " def add_columns_to_data(train_df, test_df):   \n",
        "    #instead of having two inputs out of preprocessing, edit the dataset, and add in columns which we can use as inputs\n",
        "    #we can also add an 'old' field which contains the original word \n",
        "    train_df['old'] = train_df.apply(lambda x:x['original'][x['original'].find('<')+1:x['original'].find('>')-1],axis=1)\n",
        "    test_df['old'] = test_df.apply(lambda x:x['original'][x['original'].find('<')+1:x['original'].find('>')-1],axis=1)\n",
        "\n",
        "    #first we add a field to the data which contains the edited headline\n",
        "    train_df['edited'] = train_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1],x['edit'] ) ,axis=1)\n",
        "    test_df['edited'] = test_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1], x['edit'] ) ,axis=1)\n",
        "\n",
        "    train_df['original'] = train_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1], x['old']) ,axis=1)\n",
        "    test_df['original'] = test_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1], x['old']),axis=1)\n",
        "\n",
        "    return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K1hafJCyjy-"
      },
      "source": [
        "train_df, test_df = add_columns_to_data(train_df, test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WFKfFOHqyPi"
      },
      "source": [
        "def data_to_list(data, training = True):  \n",
        "  og_headline_list = data['original'].tolist()\n",
        "  edited_headline_list = data['edited'].tolist()\n",
        "  edited_word_list = data['edit'].tolist()\n",
        "  labels_list = None\n",
        "  if training:\n",
        "    labels_list = data['meanGrade'].tolist()\n",
        "  \n",
        "\n",
        "  return og_headline_list, edited_headline_list, edited_word_list, labels_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwe8nrN0zW0x"
      },
      "source": [
        "train_og_headlines, train_edited_headlines, train_edited_words, labels = data_to_list(train_df)\n",
        "test_og_headlines, test_edited_headlines, test_edited_words, _ = data_to_list(test_df, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmDdRWrbCs7h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC7yLY8L0LHr"
      },
      "source": [
        "def preprocessing(data_list):\n",
        "  preprocessed_data = []\n",
        "\n",
        "  for sentence in data_list:\n",
        "\n",
        "    sentence = re.sub(r'[^\\w\\s\\?\\!]', '', sentence)\n",
        "    sentence = sentence.lower()\n",
        "    preprocessed_data.append(sentence)\n",
        "    \n",
        "  return preprocessed_data\n",
        "\n",
        "train_og_headlines_p = preprocessing(train_og_headlines)\n",
        "train_edited_headlines_p = preprocessing(train_edited_headlines)\n",
        "train_edited_words_p = preprocessing(train_edited_words)\n",
        "\n",
        "test_og_headlines_p = preprocessing(test_og_headlines)\n",
        "test_edited_headlines_p = preprocessing(test_edited_headlines)\n",
        "test_edited_words_p = preprocessing(test_edited_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUoiGvGvz7te",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5830468a03d5475694fc35370f6223b0",
            "c36ef91e67524463acb199c5b4bef341",
            "785dbccd9a844327b7b09e474c260af7",
            "d8d7564a801d443fa8001abb3cfcc18e",
            "c7a8221e455d4741ae5233e7bb17c3ef",
            "4d0debe500b847aea57fad925e59c129",
            "610d20bcc02949538de4c7245019a7e0",
            "996a8032758f4d84bfc8b1543ac01534"
          ]
        },
        "outputId": "96c4ec02-532d-4ed2-ddfd-7c3fb04a0ea1"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# the version that concatenates original sentences and new sentences\n",
        "train_encoded_inputs = tokenizer(train_og_headlines_p, train_edited_headlines_p, padding='max_length', max_length=90, truncation=True, return_tensors=\"pt\")\n",
        "test_encoded_inputs = tokenizer(test_og_headlines_p, test_edited_headlines_p, padding='max_length', max_length=90, truncation=True, return_tensors=\"pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5830468a03d5475694fc35370f6223b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "ebgbzfrm4idz",
        "outputId": "9713056c-95e9-4ca8-93e2-09ed2dd1844c"
      },
      "source": [
        "train_input_ids = train_encoded_inputs['input_ids']\n",
        "train_attention_mask = train_encoded_inputs['attention_mask']\n",
        "train_token_type_ids = train_encoded_inputs['token_type_ids']\n",
        "train_labels = torch.tensor(labels)\n",
        "\n",
        "train_token_type_ids[0]\n",
        "tokenizer.decode(train_input_ids.tolist()[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] pentagon claims 2000 increase in russian trolls after syria strikes what does that mean? [SEP] pentagon claims 2000 increase in russian trolls after bowling strikes what does that mean? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9brja9zLFR4"
      },
      "source": [
        "## Bert Training/Eval\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeKy-eWjF9BB"
      },
      "source": [
        "#Hyperparameters\n",
        "train_proportion = 0.8\n",
        "batch_size = 64\n",
        "lr = 5e-5\n",
        "eps = 1e-8\n",
        "epochs = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l6PAI0t4Fbq"
      },
      "source": [
        "class BERT_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x1, x2, x3, y1):\n",
        "        self.len = x1.shape[0]\n",
        "\n",
        "        self.x1_data = x1.to(device)\n",
        "        self.x2_data = x2.to(device)\n",
        "        self.x3_data = x3.to(device)\n",
        "        self.y1_data = y1.to(device)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x1_data[index], self.x2_data[index], self.x3_data[index], self.y1_data[index]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSlXJEGPQlhn"
      },
      "source": [
        "\n",
        "\n",
        "train_and_dev = BERT_Dataset(train_input_ids, train_attention_mask, train_token_type_ids, train_labels)\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,(train_examples,dev_examples))\n",
        "\n",
        "\n",
        "train_iter = DataLoader(\n",
        "            train_dataset, \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "eval_iter = DataLoader(\n",
        "            dev_dataset, \n",
        "            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "360d1c8f8f7e4ad6ae54f471d7015274",
            "4df7c46e658b4be78640a1678d15a408",
            "f2b1f3fa53a54bc883eb8ac5060bbfa4",
            "4302fdad42de41a9bedd26811876b380",
            "c5496ff15fcf47cdb013195b1ad9d8b2",
            "9abc19e9631c41d8bb47c4505dc28e80",
            "ef0a8b23a24d452fa190550ed93dec26",
            "f8d95711d1de452a9dcab4c5e890840e",
            "f591cefdde0c4c2996d6f52ec50de456",
            "635c0903f0ce4c9a8410a0c1ba2c231d",
            "e579bc2ea49b44e79f3468371ee3893d",
            "6798e20c60b74a74adaa110d7e96f9c0",
            "6031453ebb454f6799115a8d55ef54c2",
            "9ebc0c48a7a94249b3e1ad65b6ae14a6",
            "e79e711149bd47aa9165711e161ebee5",
            "57c0385dad314b648e74bb50af7d24bb"
          ]
        },
        "id": "NvvC0JgPFRyU",
        "outputId": "12af570e-cc96-48bd-a00a-2303f0fa3500"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', # the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 1,  \n",
        "    output_attentions = False, # don't return attention weights or hidden states\n",
        "    output_hidden_states = False, \n",
        ")\n",
        "model.cuda()\n",
        "#store double values\n",
        "model = model.double()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "360d1c8f8f7e4ad6ae54f471d7015274",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f591cefdde0c4c2996d6f52ec50de456",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6kGoEa-RQn7"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "\n",
        "total_steps = len(train_iter) * epochs\n",
        "# The scheduler can actually learn the best learning rate throughout tranining\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TrotOmSEG-n"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    train_losses = np.zeros(number_epoch)\n",
        "    valid_losses = np.zeros(number_epoch)\n",
        "    rmses = np.zeros(number_epoch)\n",
        "    print(\"Training model.\")\n",
        "\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        epoch_mse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for input_ids_batch, attention_mask_batch, token_type_ids_batch, target in train_iter:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # get the output\n",
        "            outputs = model(input_ids_batch,\n",
        "                            attention_mask=attention_mask_batch,\n",
        "                            token_type_ids=token_type_ids_batch)\n",
        "            \n",
        "            \n",
        "            predictions = outputs[0].squeeze(1)\n",
        "            loss = torch.sqrt(((predictions - target)**2).mean())\n",
        "           # loss, predictions = outputs[:2] \n",
        "\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "         \n",
        "            sse, mse, _ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy()) \n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse \n",
        "            epoch_mse += mse * target.shape[0]\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_mse / no_observations\n",
        "\n",
        "        train_losses[epoch-1] = epoch_loss\n",
        "        valid_losses[epoch-1] = valid_loss\n",
        "        rmses[epoch-1] = np.sqrt(epoch_mse)\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {np.sqrt(epoch_mse):.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {np.sqrt(valid_mse):.2f} |')\n",
        "    return train_losses, valid_losses, rmses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYoIP1F3EW6W"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "# We evaluate performance on our dev set\n",
        "def eval(dev_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    epoch_mse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids_batch, attention_mask_batch, token_type_ids_batch, target in dev_iter:\n",
        "\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "\n",
        "            outputs = model(input_ids_batch,\n",
        "                            attention_mask=attention_mask_batch,\n",
        "                            token_type_ids=token_type_ids_batch)\n",
        "            \n",
        "            #loss, predictions = outputs[:2] \n",
        "            predictions = outputs[0].squeeze(1)\n",
        "            loss = torch.sqrt(((predictions - target)**2).mean())\n",
        " \n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, mse, rmse = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            epoch_mse += mse*target.shape[0]\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "    \n",
        "\n",
        "    return epoch_loss/no_observations, epoch_mse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMwsi9MZEkaT"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse, rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvkEIrZJctw8"
      },
      "source": [
        "def calculate_rmse(predictions, labels):\n",
        "    loss = torch.sqrt(((predictions - labels)**2).mean())\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCArikM9cwG4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(train_losses,valid_losses,num_epochs, rmses):\n",
        "  epochs = list(range(num_epochs))\n",
        "  plt.plot(epochs,train_losses, label='train')\n",
        "  plt.plot(epochs,valid_losses, label='valid')\n",
        "  plt.legend()\n",
        "  plt.title(\"BERT Losses by Epoch\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "  plt.plot(epochs,rmses, label='train')\n",
        "  plt.legend()\n",
        "  plt.title(\"BERT RSMSE by Epoch\")\n",
        "  plt.xlabel(\"RMSE\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gwAy-noir9I",
        "outputId": "f5bf9906-f898-4253-e36e-34f1218064be"
      },
      "source": [
        "# Set the seed value all over the place to make this reproducible.\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "train_losses, valid_losses, rmses = train(train_iter, eval_iter, model, 5)\n",
        "plot(train_losses, valid_losses, len(train_losses), rmses)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.44 | Train MSE: 0.20 | Train RMSE: 0.44 |         Val. Loss: 0.55 | Val. MSE: 0.30 |  Val. RMSE: 0.55 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "EfFNl-fNNNxD",
        "outputId": "757e5618-0fda-47a3-f9b7-db8e00b7a472"
      },
      "source": [
        "\n",
        "\n",
        "epochs = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "rmse = []\n",
        "\n",
        "for e in training_stats:\n",
        "  epochs.append(e['epoch'])\n",
        "  training_loss.append(e['Training Loss'])\n",
        "  validation_loss.append(e['Valid. Loss'])\n",
        "  rmse.append(e['Valid. RMSE.'])\n",
        "\n",
        "plt.plot(epochs, training_loss, color = 'blue', label = 'training loss')\n",
        "plt.plot(epochs, validation_loss, color = 'green', label = 'validation loss')\n",
        "plt.title(\"Training and validation loss per epoch\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, rmse)\n",
        "plt.title(\"RMSE per epoch\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-fd099ebeea5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_stats' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klgk-IngaRLa"
      },
      "source": [
        "\n",
        "# Approach 2: No pre-trained representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTBXwqYqKIIB"
      },
      "source": [
        "## Building a word embedding - Gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDgubwuRXQz9"
      },
      "source": [
        "nlp = spacy.load(\"en\")\n",
        "GUTENBERG_DIR = \"drive/MyDrive/Imperial_College/NLP_CW/ic_nlp_cw/gutenberg/\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHbR38n3XrKu",
        "outputId": "34397927-6d36-4482-b57f-2509d1596e10"
      },
      "source": [
        "\n",
        "gutenberg_books = []\n",
        "for i, book_name in enumerate(os.listdir(GUTENBERG_DIR)):\n",
        "    book_file = open(os.path.join(\n",
        "        GUTENBERG_DIR, book_name), encoding=\"latin-1\")\n",
        "    book = book_file.read()\n",
        "    gutenberg_books.append(book)\n",
        "    book_file.close()\n",
        "    #if i == 1:\n",
        "    #    break\n",
        "\n",
        "gutenberg_book_lines = []\n",
        "for book in gutenberg_books:\n",
        "    book_lines = book.split(\"\\n\")\n",
        "    book_lines = list(filter(lambda x: x != \"\", book_lines))\n",
        "    print(book_lines)\n",
        "    gutenberg_book_lines.append(book_lines)\n",
        "    \n",
        "print(gutenberg_book_lines[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eloh_PpEaORC"
      },
      "source": [
        "tokenized_corpus = []\n",
        "for book_line in gutenberg_book_lines:\n",
        "    for line in book_line:\n",
        "        doc = nlp(line)\n",
        "        tokenized_corpus.append([token.text.lower()\n",
        "                                  for token in doc if not token.is_punct])\n",
        "\n",
        "print(tokenized_corpus[0:5])\n",
        "pickle.dump(tokenized_corpus, open(\"tokenized_corpus_gutenberg.pkl\", \"wb\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeyX9FPuKXF_"
      },
      "source": [
        "print(len(tokenized_corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-RTJDHieGZE"
      },
      "source": [
        "train_df['old'] = train_df.apply(lambda x:x['original'][x['original'].find('<')+1:x['original'].find('>')-1],axis=1)\n",
        "train_df['original'] = train_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1], x['old']) ,axis=1)\n",
        "\n",
        "#add original corpus\n",
        "for headline in train_df.original:\n",
        "    doc = nlp(headline)\n",
        "    tokenized_corpus.append([token.text.lower()\n",
        "                                  for token in doc if not token.is_punct])\n",
        "print(len(tokenized_corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1YwV4YAKcQz"
      },
      "source": [
        "#add edit words\n",
        "for word in train_df.edit:\n",
        "    doc = nlp(word)\n",
        "    tokenized_corpus.append(doc.text.lower())\n",
        "    \n",
        "pickle.dump(tokenized_corpus, open(\"tokenized_corpus_gutenberg.pkl\", \"wb\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xotsPqJ8ayrx",
        "outputId": "8e69bedc-ab13-4d18-e602-b82b129615d0"
      },
      "source": [
        "#maybe change cut off frequency\n",
        "def get_vocabulary(tokenized_corpus, cutoff_frequency=1):\n",
        "    vocab_freq_dict = dict()\n",
        "    for sentence in tokenized_corpus:\n",
        "        for token in sentence:\n",
        "            if token not in vocab_freq_dict.keys():\n",
        "                vocab_freq_dict[token] = 0\n",
        "\n",
        "            vocab_freq_dict[token] += 1\n",
        "\n",
        "    vocabulary = set()\n",
        "    for sentence in tokenized_corpus:\n",
        "        for token in sentence:\n",
        "            if vocab_freq_dict[token] > cutoff_frequency:\n",
        "                vocabulary.add(token)\n",
        "                \n",
        "    return vocabulary\n",
        "\n",
        "vocabulary = get_vocabulary(tokenized_corpus)\n",
        "print(\"LENGTH OF VOCAB:\", len(vocabulary), \"\\nVOCAB:\", vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LENGTH OF VOCAB: 4737 \n",
            "VOCAB: {'casts', 'act', 'dear', 'east', 'stewart', 'ruling', 'monday', 'percent', 'awkward', 'consider', 'desirable', 'doubted', 'pride', 'watched', 'win', 'merits', 'am', 'trump', 'coalition', 'mitch', 'libya', 'rolls', 'rare', 'seats', 'constant', 'ones', 'flexes', 'plot', '20-week', 'correction', 'produced', 'stopt', 'millions', 'removing', 'because', 'undo', 'paying', 'afraid', 'eldest', 'keeper', 'relative', 'chart', 'settle', 'charms', 'palmer', 'recollection', 'winner', 'boys', 'aggression', 'meeting', 'declared', 'skin', 'clinton', 'parties', 'harry', 'presented', 'although', 'kitchen', 'aware', 'asleep', 'bright', 'plant', 'overcome', 'intelligence', 'reasonable', 'edward', 'talks', 'liu', 'web', 'advanced', 'sign', 'outrage', 'banning', 'crossed', 'kansas', 'uncle', 'asked', '2017', 'brother', 'allows', 'reported', 'h.', 'show', 'delaford', 'natured', 'defends', 'pakistan', 'rally', 'assaulted', 'pounds', 'thomas', 'stream', 'asks', 'imf', 'perhaps', 'hours', 'forest', 'amazon', 'fortitude', 'counsel', 'press', 'fields', 'chelsea', 'referendum', 'movie', 'legislation', 'agreeable', 'race', 'shutdown', 'warmly', 'medicaid', 'bore', 'score', 'deaths', 'emotion', 'system', 'from', 'entire', 'judged', 'gentleman', 'paused', 'understood', 'compared', 'whenever', 'soaring', 'cia', 'mouth', 'racism', 'ukraine', 'double', 'planet', 'fairy', 'argument', 'hard', 'prevented', 'canada', 'tensions', 'few', 'san', 'metoo', 'higher', 'sleep', 'means', 'result', 'near', 'beating', 'bought', 'forward', 'feet', 'unkind', 'prevent', 'white', 'change', 'consequences', 'fearful', 'dilemma', 'declines', 'dhs', 'sheep', 'deliver', 'racial', 'kasich', 'brothers', 'japanese', 'integrity', 'middle', 'expensive', 'senators', 'those', 'politeness', 'communicated', 'hitherto', 'chairman', 'account', 'tenderness', 'continues', 'firm', 'nicest', 'next', '16', 'laid', 'endorses', 'settled', 'national', 'weapons', 'game', 'restless', 'consequence', 'mccain', 'warring', 'christie', 'economists', 'problem', 'exist', 'increasing', 'bestow', 'van', 'every', 'navy', 'resigns', 'repeal', 'military', 'temptation', 'people', 'attached', 'maduro', 'sides', 'folly', 'hint', 'ugly', 'greeted', 'mood', 'deficit', 'nra', 'pit', 'intimacy', 'years', 'dashwoods', 'galore', 'hack', 'housekeeper', 'trigger', 'giuliani', 'delivered', 'relief', 'trading', 'election', 'there', 'seed', 'fraud', 'elsewhere', 'mentioned', 'throwing', '19', 'fighter', 'territory', 'bars', 'too', 'enough', 'submitted', 'crimes', 'threatens', 'backlash', 'treated', 'violence', 'delusional', 'research', 'addition', 'elsa', 'side', 'an', 'wondered', 'ill', 'speech', 'notes', 'brooklyn', 'suspects', 'persuaded', 'overall', 'resolution', 'acting', 'cuts', 'thousand', 'preparing', 'big', 'deed', 'excess', 'boiling', 'kick', 'union', 'exempts', 'tour', 'amount', 'trafficked', 'barack', 'away', 'revealing', 'shoulder', 'stormy', 'where', 'extremists', 'links', 'viewing', 'upset', 'tomorrow', 'jfk', 'itself', 'shiny', 'delighted', 'darkness', 'lock', 'taken', 'ignorance', 'trundle', 'improved', 'challenging', 'needed', 'heightening', 'smallest', 'defend', 'mutual', 'confused', 'felt', 'three', 'proud', 'thin', 'bag', 'co', 'seeing', 'restaurant', 'mandatory', 'hesitation', 'lines', 'wished', 'peninsula', 'whither', 'blunt', 'headlines', 'oath', 'meets', 'ford', 'beloved', 'portland', 'biggest', 'detained', 'malaysia', 'murray', 'features', 'increase', 'rope', 'impossible', 'arrest', '6', 'besides', 'handsome', 'feared', '10,000', 'die', 'turned', 'since', 'hermit', 'blocks', 'announce', 'here', 'sing', 'modest', 'mattis', 'crimea', 'relations', 'touch', 'ould', 'moderate', 'position', 'failure', 'timed', 'lee', 'google', 'g.o.p.', 'liberalism', 'outcry', 'fictional', 'division', 'orchard', 'hunting', 'garraun', 'lips', 'sufferings', 'worried', 'amazement', 'with', 'resolving', 'form', 'assembly', 'art', 'hotel', 'politician', 'ass', 'breaking', 'branches', 'legislature', 'read', 'drew', 'full', 'forth', 'complete', 'react', 'presidents', 'gps', 'oks', 'heard', 'built', 'submission', 'trucker', 'faint', 'forty', 'residence', 'project', 'awards', 'dnc', 'candidates', 'writing', 'fema', 'caravan', 'tail', 'useless', 'iraq', 'followed', 'chinese', 'purpose', 'crime', 'camp', 'considerable', 'conduct', 'cousins', 'runs', 'fool', 'island', 'desirous', 'endangered', 'send', 'eu', 'tower', 'memory', 'country', 'altogether', 'consolation', 'good', 'violently', 'sun', 'fresh', 'entirely', 'tone', 'comment', 'campaign', 'maher', 'soldiers', 'g-20', 'bed', 'cried', 'cory', 'observing', 'mick', 'hanging', 'radishes', 'attack', 'unceasing', 'legend', 'international', 'dreamer', 'lynch', 'indifferent', 'abuse', 'independent', 'ahead', 'ease', 'palace', 'pentagon', 'president', 'block', 'cases', 'unity', 'boris', 'includes', 'bread', 'unless', 'toilet', 'yellow', 'elves', 'carried', 'no', 'sudan', 'perceived', 'concluded', 'both', '10', 'dc', 'early', 'turkey', 'queen', 'presidency', 'truce', 'captive', 'heated', 'maddow', 'unfit', 'monkey', 'the', 'likewise', 'gut', 'threw', 'street', 'dislike', 'journey', 'strike', 'may', 'dream', 'charles', 'spying', 'r', 'appearance', 'selfishness', 'voters', 'days', 'slowly', 'daughters', 'as', 'attempt', 'ask', 'heavy', 'congress', 'without', 'shell', 'perfectly', 'denies', 'showdown', 'dressed', 'hotter', 'session', 'pair', 'balls', 'largest', 'alarm', 'sessions', 'particularly', 'census', 'adam', 'companion', 'shop', 'panel', 'forgot', 'cook', 'explanation', 'broke', 'fantasy', 'moreover', 'plainly', 'chair', 'conjecture', 'declares', 'transparent', 'readily', 'dreamers', 'dad', 'communication', 'fascist', 'behaved', 'virginia', 'seven', 'wine', 'emperor', 'bring', 'cleveland', 'wounds', 'maid', 'bombing', \"'ll\", 'nearest', 'confidence', 'journalist', 'opponents', 'greece', 'interference', 'crying', 'hackers', 'warner', 'liar', 'black', 'happier', 'believed', 'tiny', 'threaten', 'cover', 'dried', '12', 'africa', 'beg', 'dog', 'realizing', 'laws', 'obscene', 'reason', 'blow', 'solicitude', 'letting', 'mission', 'corrects', 'created', 'text', 'go', 'shape', 'schultz', 'pledge', 'sent', 'compassion', 'forced', 'kicked', 'brandon', 'entering', 'chickens', 'marijuana', 'patients', 'died', 'triumph', 'less', 'confined', 'escaped', 'associated', 'sincerity', 'saves', 'frode', 'retreat', 'sake', 'investors', 'cbo', 'endeavoured', 'collapse', 'classified', 'israelites', 'assad', 'life', 'warren', 'trap', 'indeed', 'militants', 'english', 'wish', 'fourth', 'attacked', 'elections', 'commander', 'fired', 'love', 'emails', 'mcmaster', 'process', 'israeli', 'natural', 'remarkably', 'dorsetshire', 'arab', 'totally', 'focused', 'partners', 'bird', 'asylum', 'scott', 'subpoenas', 'running', 'further', 'serving', 'moon', 'philistine', 'probe', 'gods', 'lucy', 'poisoning', 'jobs', 'popular', 'happened', 'promise', 'neck', 'lose', 'short', 'escape', 'addresses', 'prayer', 'brexit', 'assure', 'picked', 'equipment', 'visa', 'threatened', 'supply', 'strength', 'mystery', 'atlantic', 'stock', 'motives', 'dashwood', 'warning', 'resolve', 'banks', 'locked', 'reckoning', 'false', 'difficulty', 'how', 'hen', 'xl', 'npr', 'undercuts', 'wild', 'tens', 'warmth', 'hat', 'fallout', 'indonesia', 'existing', 'replace', 'be', 'news', 'fox', 'completely', 'bartlett', 'bold', 'refusal', 'nigerian', 'increased', 'dearest', 'dragged', 'witch', 'raises', 'ai', 'gives', 'doug', 'updates', 'quietly', 'nugent', 'working', 'picture', 'hates', 'size', 'venezuela', 'warns', 'robert', 'frightened', 'holy', 'admin', 'wh', 'conditions', 'introduce', 'total', 'possibly', 'stone', 'imports', 'sources', 'neutrality', 'porn', 'hospital', 'imprudence', 'fall', 'affair', 'corn', 'extent', 'merit', 'would', 'intentions', 'deeper', 'valued', 'tough', 'students', 'eat', 'problems', 'buck', 'services', 'fright', 'philippines', 'refuses', 'affairs', 'bow', 'fish', 'seems', 'middletons', 'considering', 'instead', '5', 'cleared', 'capitol', 'wrestling', 'heels', 'used', 'delayed', 'dick', 'horror', 'martin', 'long', 'nightingale', 'delight', 'reward', 'epa', 'stop', 'heaven', 'fingers', 'stands', 'excuse', 'hills', 'un', 'pastor', 'prepares', 'fled', 'considers', 'object', 'accounts', 'communicate', 'author', 'bathroom', 'prime', 'hostility', 'nafta', 'al', 'trial', 'oprah', 'eclipse', 'hears', 'far', 'endeavouring', 'backed', 'pratt', 'disposition', 'stole', 'colonel', 'insurance', 'ashamed', 'killing', 'radish', 'herself', 'by', 'knowing', 'remember', 'language', 'usual', 'constantly', '60', 'critic', 'condition', 'temporary', 'plough', 'apology', 'deeply', 'lettuce', 'hopeless', 'felony', 'men', 'patrick', 'earnestly', 'shelter', 'opposed', 'resistance', 'longstaple', 'welfare', 'market', 'neighbourhood', 'uk', 'ordering', 'down', '23', 'quits', 'berlin', 'jury', 'scottish', 'nuclear', 'final', 'way', 'chemical', 'confront', 'pm', 'you', 'guide', 'killer', 'broth', 'magic', 'fruit', 'placed', 'offices', 'cohen', 'improving', 'grounds', 'merkel', 'played', 'arm', 'devin', 'pardons', 'deer', 'n’t', 'briefings', 'publisher', 'dare', 'laughing', 'cash', 'supposed', 'heroin', 'twice', 'clue', 'giant', 'rows', 'irs', 'firmness', 'lockdown', 'pianoforte', 'green', 'divide', 'latest', 'shelf', 'trapped', 'depends', 'learn', 'opening', 'fisa', 'impatient', 'rosenstein', 'necessary', 'longed', 'rape', 'famous', 'most', 'waters', 'parents', 'explain', 'pleasure', 'facebook', 'sanders', 'vows', 'everything', 'began', 'moving', 'someone', 'andrew', 'ministers', 'blame', 'king', 'blast', 'western', 'dollar', 'investigations', 'modern', 'improvement', 'connections', 'accuses', 'pennsylvania', 'luther', 'shirt', 'michigan', 'anxious', 'nfl', 'bosses', 'articles', 'poland', 'truck', 'charge', 'prosecutors', 'scientific', 'sisters', 'submarine', 'worry', 'fancied', 'terms', 'spent', 'demands', 'calmly', 'pokes', 'wildly', 'stones', \"'re\", 'safeguards', 'guilty', 'ordinary', 'wins', 'blames', 'cutting', 'elegance', 'thankful', 'loan', 'bushes', 'model', 'parlour', 'deficits', 'faster', 'unwilling', 'according', 'expelled', 'attitude', 'law', 'sound', 'superior', 'indifference', 'foot', 'skeptical', 'religious', 'try', 'wood', 'ripe', 'sexual', 'conspiracy', 'ap', 'forgive', 'pyongyang', 'harrison', 'business', 'enjoy', \"'s\", 'leader', 'airlines', 'takeover', 'valley', 'romney', 'drilling', 'grateful', 'costs', 'argue', 'capture', 'gingerbread', 'join', 'agreements', '400', 'catalonia', 'added', 'formally', 'pick', 'workers', 'gillespie', 'impact', '2020', 'ad', 'pieces', 'cloak', 'gentle', 'thoroughly', 'believes', 'papa', 'major', 'niece', 'our', 'sorry', 'founded', 'quite', 'revenge', 'happiness', 'treasure', 'ship', 'senator', 'attacker', 'pelosi', 'manager', 'neo', 'comments', 'bet', 'injured', 'movement', 'eric', 'bells', '202', 'sum', 'huge', 'scarcely', 'often', 'alter', 'spain', 'stay', 'resist', 'continuing', 'girls', 'comfort', 'knowledge', 'meaning', 'bodies', 'measures', 'unpleasant', 'scandal', 'raise', 'existence', 'haspel', 'america', 'beijing', 'gifts', 'book', 'whitwell', 'reserve', 'likely', 'study', 'bernie', '88', 'package', 'encouraging', 'seth', 'government', 'ivanka', '     ', 'kentucky', 'exclusive', 'candles', 'cruel', 'hid', 'gas', 'feds', 'resign', 'continual', 'remained', 'like', 'cure', 'silver', 'owed', 'taxes', '15', 'opens', 'post', 'recommended', 'fat', 'objections', 'dry', 'rent', 'work', 'such', 'wept', 'eating', 'shock', 'royal', 'verge', 'exec', 'tom', 'talking', 'happens', 'happiest', 'nor', 'produce', 'curiosity', 'disclosure', 'red', 'deputy', 'clearly', 'blew', 'anymore', 'ordered', 'industry', 'avoided', 'pleads', 'split', 'sudanese', 'acquaintance', 'notion', 'inappropriate', 'unnecessary', 'remain', 'office', 'snow', 'reince', 'endeavour', 'road', 'shows', 'haley', 'observe', 'looking', 'taste', 'quit', 'walks', 'dread', 'sounded', 'official', 'finance', 'billionaire', 'acknowledges', 'daunting', 'unhappy', 'joined', 'distinguished', 'unkindness', 'respectable', 'politics', 'odds', 'joke', 'wind', 'afforded', 'shook', 'requiring', 'reflect', 'pleased', 'palmers', 'turkish', 'passes', 'pass', 'interview', 'also', 'dispute', 'quitting', 'named', 'entry', 'among', 'schumer', 'houses', 'started', 'afghanistan', 'targeting', 'actress', 'mayor', 'speak', 'explains', 'invitation', 'chocolate', 'secured', 'signify', 'harley', 'kennedy', 'improper', 'jones', 'last', 'murdered', 'gianforte', 'broken', 'explosive', 'ariana', 'hall', 'supporter', 'decision', 'enquire', 'cap', 'changed', 'spoken', 'partial', 'heavily', 'satisfy', 'through', 'hidden', 'approved', 'haste', 'miller', 'see', 'putting', 'dems', 'leads', 'police', 'fallen', 'buy', 'drum', 'accusations', 'impeach', 'focus', 'meet', 'fashion', 'proportion', 'following', 'ireland', 'should', 'drawn', 'hop', 'phoenix', 'term', 'wing', 'alive', 'maryland', 'ceiling', 'avert', 'difficult', 'doors', 'network', 'family', 'clothes', 'combe', 'none', 'possible', 'kindness', 'retirement', 'leaks', 'contain', 'remembrance', 'probably', 'races', 'ability', 'slaughter', 'secrecy', 'jane', 'palestinians', 'jobless', 'democratic', 'elegant', 'maidens', 'sam', 've', 'loved', 'idea', 'brown', 'removed', 'korea', 'voter', 'recruits', 'shift', 'precious', 'slams', 'season', 'dull', 'scalise', \"an't\", 'prison', 'signals', 'nations', 'sword', 'cake', 'liberals', 'intended', 'contradicting', 'christ', 'abortions', 'odd', 'occurred', 'disappointment', 'generally', 'request', 'wondering', 'back', 'engaging', 'dropped', 'raids', 'stretched', 'gave', 'fueled', 'effects', 'spies', 'role', 'unstable', 'london', 'kaspersky', 'senior', 'beginning', 'jester', 'gold', 'million', 'defense', 'music', 'los', 'emmanuel', 'undocumented', 'dark', 'spoke', 'intimate', 'ever', '11', 'worries', 'reza', 'capital', 'prudence', 'crab', 'operation', 'cooperate', 'animated', 'figure', 'eager', 'consent', 'prayers', 'swedish', 'passengers', 'mostly', 'attend', 'reform', 'had', 'upon', 'amendment', 'intimidated', 'castle', 'though', 'mike', 'response', 'michelle', 'duterte', 'breakfast', 'play', 'distrust', 'use', 'artist', 'nebraska', 'economist', 'sorrows', 'listen', 'alex', 'group', 'having', 'melania', 'politically', 'formed', 'body', 'appearing', 'grief', 'insensible', 'custody', 'actually', '2018', 'travel', 'friend', 'fueling', 'issued', 'approbation', 'shake', 'note', 'firing', 'scientists', 'cars', 'concerned', 'coat', 'georgia', 'sooner', 'recollecting', 'sort', 'college', 'religion', 'airport', 'lost', 'basic', 'expectations', 'wrote', 'clear', 'traveller', 'ducks', 'program', 'betrayed', 'animals', 'ocean', 'criminal', 'american', 'deal', 'vox', 'comfortably', 'cup', 'uber', 'yourselves', 'each', 'seem', 'walls', 'la', 'climate', 'favour', 'city', 'breitbart', 'became', 'showing', 'option', 'continuance', 'julian', 'protects', 'ye', 'animation', 'cane', 'amusement', 'crowns', 'table', 'spread', 'paw', 'email', 'drove', 'missouri', 'seat', 'political', 'occur', 'charlottesville', 'across', 'control', 'lift', 'puzzled', 'objects', 'at&amp;t', 'avoid', 'bitch', 'mud', 'grind', 'appalling', 'dodge', 'humour', 'human', 'corner', 'condemned', 'zealand', 'pompeo', 'furniture', 'assured', 'expense', 'thee', 'domestic', 'thank', 'animal', 'countenance', 'off', 'special', 'more', 'americans', 'blockbuster', 'cares', 'eagerness', 'cause', 'stairs', 'nearer', 'asia', 'approve', 'strikes', 'revise', 'roots', 'wary', 'comprehend', 'agencies', 'take', 'fire', 'months', 'engaged', 'drain', 'ads', 'paul', 'clapper', 'york', 'landmark', 'rejection', 'mulvaney', 'questioned', 'papers', 'suit', 'ceased', 'carrying', 'congressional', 'democrats', 'public', 'driver', 'indisposition', 'midst', 'ended', 'allenham', 'nukes', 'calais', 'painful', 'certified', 'chance', 'continue', 'hot', 'tis', 'sunk', 'iran', 'contacts', 'whispered', 'destroy', 'noah', 'justice', 'bump', 'page', 'brahmin', 'regulators', '’ll', 'staying', 'mamma', 'regrets', 'led', 'irresistible', 'course', 'quarter', 'suspends', 'reporters', 'arpaio', 'editor', 'begins', 'sending', 'assassination', 'shewn', 'dem', 'shocking', 'divided', 'couple', 'jr', 'cambridge', 'concern', 'boycott', 'very', 'girl', 'confusion', 'identity', 'bomb', 'certainly', 'manners', 'suffering', 'star', 'steady', 'amid', 'stronger', 'effect', 'fell', 'pharma', 'unable', 'yesterday', 'jungle', 'under', 'planning', 'charged', 'attention', 'observation', 'bear', 'waiting', 'this', 'brings', 'gorsuch', 'teachers', 'meant', 'deserve', 'conscience', 'misery', 'laden', 'intel', 'ousted', 'incompetent', 'conservatives', 'encouragement', 'strong', 'conway', 'christmas', 'taiwan', 'starts', 'flooding', 'disposed', 'rnc', 'invited', 'urges', 'wayne', 'sat', 'michael', 'wage', 'gift', 'singing', 'nuke', 'johnson', 'tourists', 'spite', 'testimony', 'sussex', 'just', 'smart', 'cousin', 'families', 'dismiss', 'dozens', 'temper', 'anne', 'ellison', 'mill', 'drops', 'tapped', 'eyes', 'within', 'doctor', 'helps', 'closet', 'target', 'iowa', 'd.c.', 'aged', 'once', 'languid', 'inevitable', 'warned', 'agrees', 'grown', 'trunk', 'between', 'part', 'tied', 'oh', 'wishing', 'arose', 'bar', 'carlson', 'advisors', 'learned', 'all', 'sight', 'closed', 'attacking', 'hong', 'mere', 'minnesota', 'veterans', 'claimed', 'hoppity', 'chances', 'reaction', 'putin', 'confirms', 'acorn', 'happily', 'review', 'rt', 'slept', 'merry', 'suicide', 'inconstancy', 'declare', 'expression', 'invites', 'describe', 'earth', 'their', 'joy', 'delicacy', 'dying', 'mr', \"'m\", 'noise', 'variety', 'break', 'tap', 'mass', 'behavior', 'staffers', 'transgender', 'polluted', 'order', 'devos', 'refugee', 'rico', 'turns', 'roll', 'margaret', 'christians', 'found', 'india', 'else', 'tiffany', 'property', 'throw', 'caught', 'cow', 'makes', 'letters', 'farmer', 'playing', 'grande', 'composure', 'warm', 'credit', 'mighty', 'feelings', 'guilt', '30', 'zinke', 'degree', 'wanted', 'twelve', 'theresa', 'ay', 'draw', 'angel', 'surprise', 'desired', 'immediately', 'biden', 'performed', 'do', 'till', 'spirits', 'imagination', 'add', 'on', 'parted', 'explained', 'collection', 'extreme', 'proved', 'soil', 'walk', 'gray', 'steve', 'agreement', 'rebels', 'suffer', 'wife', 'desire', 'circumstances', 'fun', 'accepted', 'points', 'rejoiced', 'opposition', 'gratifying', 'ceremony', 'facing', 'science', 'crackdown', 'scathing', 'colour', 'heading', 'debts', 'silencing', 'projects', 'intercepted', 'improve', 'stars', 'flake', 'criticism', 'raid', 'drugs', 'denial', 'punishment', 'extraordinary', 'ranked', 'second', 'reforms', 'kamala', 'oliver', 'declining', 'homeless', 'trusted', 'rice', 'bans', 'liberality', 'seriously', 'turning', 'hurried', 'steeles', 'ingraham', 'real', 'silently', 'poll', 'matters', 'sir', 'plans', 'parliament', 'lie', 'australian', 'arkansas', 'appears', 'sensibility', 'farm', '100', 'girlfriend', 'probable', 'resolved', 'eastern', 'ski', 'offending', 'channels', 'passion', 'chamberlain', \"'d\", 'opportunity', 'rather', 'accepting', 'drinking', 'golf', 'obama', 'cable', 'youngest', 'pressured', 'gov', 'estate', 'step', 'nine', 'regret', 'pause', 'around', 'color', 'aide', 'disapproval', 'jeremy', 'wings', 'emerge', 'china', 'fellow', 'independence', 'gone', 'nature', 'it', 'undisclosed', 'causing', 'moved', 'u.s.', 'socialist', 'larger', 'suitable', 'forcing', '22', 'kimmel', 'unfortunately', 'steel', 'temporarily', 'praise', 'tracking', 'leadership', 'insult', 'string', 'border', 'teenage', 'winning', 'uninsured', 'strategy', 'designs', 'wealth', 'benefit', 'spring', 'approach', 'continued', 'requesting', 'sean', 'reject', 'miss', 'calm', 'supper', 'stuck', 'minute', 'jets', 'power', 'arrested', 'columnist', 'room', 'disaster', 'devonshire', 'source', 'analytica', 'university', 'wants', 'looked', 'important', 'kelly', 'village', 'restore', 'pope', 'shithole', 'journalists', 'rocket', 'carpet', 'wrong', '45', 'blind', 'hole', 'crack', 'horses', 'accept', 'vladimir', 'trees', 'o', 'oust', 'comes', 'report', 'rates', 'teach', 'little', 'gottlieb', 'boost', 'careful', 'sense', 'threshers', 'starting', 'summit', 'pursued', 'mrs.', '21st', 'quickly', 'sales', 'reopens', 'barred', 'point', 'square', 'recover', 'forgiveness', 'meetings', 'luxury', '28', 'gently', 'super', 'war', 'thought', 'necessity', 'gentlemen', 'plays', 'mind', 'thing', 'candidate', 'connection', 'roadside', 'forecast', 'politicians', 'video', 'mention', 'thirty', 'seoul', 'dignity', 'legalized', 'scared', 'civil', 'up', 'tempt', 'slump', 'central', 'nominates', 'trade', 'parks', 'finished', 'fighting', 'helping', 'actions', 'seattle', '|', 'write', 'decisions', 'pundit', 'lived', 'trust', 'jokes', 'got', 'immigrant', 'distressed', 'crazy', '1.5', 'round', 'church', 'palm', 'seek', 'confirming', 'sufficient', 'propose', 'bill', 'u.n.', 'loyal', 'sitting', 'baby', 'delay', 'beside', 'for', 'late', 'scheme', 'terrible', 'comparison', 'prepare', 'pop', 'da', 'protected', 'c.i.a.', 'belief', 'pursuits', 'responsibility', 'plenty', 'god', 'cross', 'rhetoric', 'offend', 'eve', 'woods', 'recovered', 'brazil', 'nod', 'shure', 'warn', 'pleasing', 'delicate', 'belonging', 'debacle', 'hud', 'wedding', 'maps', 'smash', 'sees', 'chaise', 'portrait', 'cloud', 'strengthen', 'guns', 'himself', 'upcoming', 'frequently', 'health', 'need', 'punish', 'end', 'orleans', 'out', 'contradictions', 'absence', 'dearer', 'rocks', 'consumer', 'wheat', 'trucks', 'determine', 'ridiculous', 'tale', 'fine', 'admitting', 'aid', 'hurricane', 'mixed', 'moscow', 'value', 'checks', 'treat', 'career', 'january', 'softly', 'coast', 'discrimination', 'kiss', 'iranian', 'prices', 'miserable', 'bad', 'visit', 'donkey', 'hardly', 'school', 'materially', 'measure', 'allegedly', 'resentment', 'trumpcare', 'critical', 'guy', 'honored', 'elected', 'same', 'senate', 'experience', 'mired', 'move', 'worthless', 'republican', 'tame', 'bubbles', 'younger', 'attentions', 'canadian', 'options', 'true', 'sister', 'gina', 'obamacare', 'donald', 'wire', 'inquiries', 'medicare', 'hearted', 'establishment', 'ends', 'sicily', 'wore', 'permit', 'countries', 'large', 'details', 'begin', 'remark', 'south', 'provoke', 'commencement', 'coup', 'thus', 'compliance', 'rule', 'accommodation', 'bust', 'plain', 'degrees', 'taking', 'mountain', 'interviews', 'prudent', 'terror', 'bless', 'speaker', 'shepherd', 'pew', 'issues', 'washington', 'holds', 'inauguration', 'community', 'attending', 'heartily', 'resemblance', 'convincing', 'anniversary', 'reading', 'wonder', 'throne', 'pained', 'eligible', 'puppet', ' ', 'souls', '70', 'islamist', 'seeking', 'rain', 'succeeding', 'suffered', 'tense', 'bashing', 'willoughby', 'feels', 'sticks', 'twitter', 'one', 'assault', 'incumbent', 'louisiana', 'promising', 'giants', 'eighteen', 'hundreds', 'doing', 'ventured', 'held', 'financial', 'ready', 'diplomats', 'dine', 'woodpecker', 'them', 'open', 'compliment', 'monster', 'unlikely', 'stolen', 'oval', 'plan', 'returned', 'spot', 'falling', 'james', 'appointed', 'flock', 'programs', 'fifty', 'except', 'mess', 'engagement', 'tea', 'complex', 'beautiful', 'lower', 'child', 'especially', 'fit', 'bringing', 'dancing', 'took', 'afterwards', 'propriety', 'force', 'dinesh', 'jason', 'heat', 'enjoyment', 'assurances', 'great', 'aides', 'gingrich', 'stadiums', 'visitors', 'attempted', 'settling', 'pursuing', 'thoughts', 'suspect', 'thy', 'announcement', 'colbert', 'tony', 'constitutional', 'tears', 'slight', 'follow', 'culture', 'doj', 'crept', 'stealing', 'volunteer', 'known', 'writer', 'quiet', 'of', 'donors', 'chick', 'activists', 'beasts', 'profession', 'pressed', 'sink', 'pre', 'flattery', 'sugar', 'experiment', 'positive', 'butter', 'waste', 'lion', 'hold', 'crisis', 'sermon', 'doo', 'appear', 'resignation', 'progressives', 'weeks', 'everyone', 'afford', 'hopes', 'suggests', 't', 'guess', 'inspired', 'norland', 'environment', 'syria', 'controversial', 'closer', 'puppy', 'tips', 'jake', 'graham', 'tolerably', 'whether', 'calls', 'kim', 'nobel', 'ears', 'today', 'evil', 'find', 'lawyers', 'launches', 'math', 'danced', 'venture', 'counter', 'fairies', 'two', 'robes', 'interesting', 'testify', 'field', 'testifies', 'economic', 'explosion', 'palestinian', 'own', 'excessively', 'abroad', 'favourable', 'lay', 'secretly', 'naked', 'probability', '50', 'era', 'unit', 'receives', 'dakota', 'charity', 'mitt', 'shoots', 'voting', 'discourse', 'george', 'louisa', 'investigating', 'secretary', 'become', 'pretty', 'cuba', 'rant', 'broad', 'choice', 'perfect', 'clothing', 'richard', 'friendly', 'festival', 'express', 'heavenly', 'partner', 'left', 'ballistic', 'aslan', 'went', 'judge', 'host', 'latin', 'failing', 'believe', 'reasons', 'ceos', 'strongest', 'incident', 'bigger', 'exclaimed', 'officials', 'affectionate', 'facts', 'finalists', 'paid', 'these', 'killed', 'tweet', 'class', 'charlotte', 'shots', 'inside', 'favorite', 'engagements', 'oppressed', 'reception', 'honoured', 'yes', 'latter', 'well', 'walked', '4', 'mccabe', 'job', 'affected', 'admire', 'partisanship', 'basically', 'fence', 'bye', 'egg', 'refused', 'n.korea', 'drunken', 'tourist', 'shoemaker', 'offended', 'shocked', 'convicted', 'loses', 'fights', 'beaux', 'seemed', 'dreadful', 'otherwise', 'crossing', 'steele', 'funded', 'hour', 'growing', 'man', 'husband', 'coming', 'lying', 'defending', 'foreign', 'terrorism', 'companies', 'vote', 'british', 'stand', 'scaramucci', 'length', 'grenfell', 'hope', 'marked', 'mosque', 'uncommonly', 'state', 'cnn', 'sad', 'seizes', 'imminent', 'madam', 'contrary', 'restricting', 'scale', 'week', 'month', 'n', 'russian', 'wolf', 'listened', 'garbage', 'cabinet', 'cruise', 'populist', 'excited', 'period', 'plea', 'understand', 'campus', 'female', 'operative', 'banned', 'shadowy', 'jail', 'muscle', 'ways', 'attorney', 'kettle', 'duty', 'judgment', 'my', 'justified', 'beau', 'but', 'ave', 'backs', 'thailand', 'barton', 'director', 'nay', 'fined', 'gates', 'pin', '53', 'conversation', 'pen', 'isis', 'merely', 'owe', 'margery', 'either', 'easily', 'offered', 'year', 'suspicious', 'knees', 'sea', 'shooter', 'radio', 'bit', 'arabia', 'cruz', 'west', 'rich', 'protection', 'worse', 'lit', 'startling', 'list', 'app', 'ignored', 'warriors', 'beast', 'blueprint', 'blackmail', 'david', 'beetle', 'let', 'adopt', 'packages', 'contact', 'pain', 'fixed', 'falls', 'humoured', 'california', 'saved', 'they', 'dissatisfied', 'passing', 'opinions', 'dirty', 'drop', 'laughter', 'secret', 'warships', 'skinny', 'murder', 'enter', 'mammy', 'usually', 'soul', 'protections', 'taxpayer', 'griffin', 'miles', 'pocket', 'record', 'soldier', 'path', '100,000', 'lgbtq', 'linking', 'hear', 'rank', 'lives', 'eight', 'zimbabwe', 'sit', 'online', 'reflections', 'policies', \"n't\", 'weekly', 'netanyahu', 'fugitive', 'piccola', 'pregnancy', 'joe', 'into', 'las', 'times', 'patient', 'courage', 'leaving', 'magna', 'alteration', 'hhs', 'expand', 'fetch', 'hannity', 'prince', 'endorsement', 'fcc', 'march', 'mouse', 'abolish', 'presently', 'pulse', 'statements', 'junk', 'condemns', 'top', 'tells', 'forget', 'conclusion', '2016', 'graffiti', 'grassley', 'cool', 'knows', 'lead', 'motive', 'analysis', 'soros', 'mulls', 'french', 'current', '      ', 'forgiven', 'making', 'were', 'borne', 'racist', 'views', 'indian', 'question', 'suing', 'aimed', 'scientist', '101', 'certain', 'view', 'then', 'separate', 'jinping', 'surveillance', 'voted', 'waffle', 'anthem', 'melissa', 'camps', 'money', 'executive', 'conceal', 'cyber', 'contributions', 'snubs', 'repeated', 'rep', 'flight', 'readies', 'leaked', 'employed', 'start', 'commissioner', 'complaint', 'speaking', 'his', 'creature', 'increasingly', 'jerry', 'grew', 'expressed', 'museum', 'window', 'corbyn', 'visits', 'thou', 'performance', 'opec', 'struck', 'proceed', 'reluctantly', 'disgrace', 'armed', 'anthony', 'revises', 'are', 'drug', 'provision', 'williams', 'russians', 'cheerfulness', 'think', 'described', 'met', 'combat', 'thaad', 'communications', 'blocked', 'tweeted', 'worth', 'medal', 'remaining', 'fires', 'plants', 'nominees', 'wound', 'muslim', 'unjust', 'light', 'trying', 'puts', 'missile', 'chris', 'correct', 'hide', 'young', 'leaning', 'spare', 'replacement', 'perch', 'softened', 'new', 'favor', 'given', 'olympic', 'phone', 'statues', 'mean', 'sweet', 'affect', 'frequent', 'daily', 'sudden', 'appoint', 'targets', 'pointed', 'whence', 'history', 'branding', 'four', 'abortion', 'altered', 'interrupted', 'answer', 'direct', 'events', 'lap', 'visited', 'similar', 'responds', 'criminalizing', 'players', 'agent', 'labor', 'trip', 'count', 'sinking', 'toyota', 'debt', 'w.', 'self', 'test', 'hawaii', 'collusion', 'rate', 'ladyship', 'auction', 'daughter', 'susan', 'evident', 'explaining', 'polls', 'aside', 'jared', 'towards', 'sparks', 'spanish', 'complexion', 'missed', 'shore', 'former', 'vice', 'directly', 'singer', 'parkland', 'gained', 'distant', 'joining', 'tune', 'data', 'lacking', 'letter', 'however', 'clever', 'investigators', 'violent', 'pruitt', 'leading', 'conflicts', 'shut', 'old', 'collude', 'bannon', 'awake', 'agony', 'impertinent', 'comey', 'affection', 'morning', 'gardens', 'ride', 'droll', 'artificial', 'release', 'exertion', 'television', 'disposal', '17', 'lobbyists', 'treasury', 'bound', 'britain', 'threat', '16-year', 'third', 'secrets', 'pays', 'cooperation', 'u.k.', 'solomon', 'photos', 'absolute', 'inviting', 'models', 'weight', 'wicked', 'governor', 'age', 'effort', 'allowing', 'surely', 'fully', 'downs', 'building', 'different', 'shadow', 'vanity', 'picturesque', '2', 'sometimes', 'constitution', 'ire', 'personal', 'presence', 'name', 'bade', 'imagine', 'income', 'probes', 'summer', 'present', 'clouds', 'selfish', 'weighs', 'jeff', 'neither', 'sons', 'trumped', 'nancy', 'card', 'misconduct', 'house', 'mad', 'olympics', 'water', 'electoral', 'davos', 'values', 'over', 'highly', 'easter', 'separated', 'council', 'including', 'twenty', 'earned', 'fate', 'fact', 'fanny', 'windows', 'conference', 'handling', 'nkorea', 'beauty', 'weather', 'monsanto', 'brought', 'sends', 'hoping', 'supporters', 'saw', 'perceiving', 'bizarre', 'enforcement', 'trouble', 'greater', 'corruption', 'mercer', 'media', 'catastrophe', '20', 'dagda', 'approves', 'halls', 'prospect', 'zelia', 'becoming', 'separation', 'must', 'seeds', 'indonesian', 'deadly', 'shoes', 'fir', 'corporate', 'bare', 'alert', 'feel', 'striking', 'expected', 'protesters', 'plastic', 'flee', 'severe', 'loud', 'acknowledge', 'loves', 'tuesday', 'ryan', 'insider', 'century', 'lot', 'deported', 'cheese', 'xi', 'sale', 'setting', '8', 'gratitude', 'delhi', 'suddenly', 'spirit', 'occasion', 'goodness', 'offer', 'startled', 'able', 'unveil', 'gov.', 'smooth', 'fbi', 'betsy', 'australia', 'that', 'tackle', 'moment', 'care', 'shouting', 'violation', 'fomorians', 'apartment', 'several', 'conflict', 'squeeze', 'parting', 'arizona', 'firms', 'five', 'regarded', 'concentration', 'discovery', 'extremely', 'time', 'pictures', 'clock', 'dewdrop', 'paris', 'qatar', 'works', 'kushner', 'best', 'out,--', 'slow', 'victims', 'earlier', 'hoped', 'seal', 'diversity', 'shooting', 'dangerous', 'migrant', 'porter', 'vegas', 'agency', 'activity', 'hugs', 'collision', 'pray', 'shared', 'european', 'hate', 'nominee', 'coffee', 'hit', 'companions', 'headquarters', 'theorist', 'investigation', 'recollect', 'lake', 'rex', 'crooked', 'destroyed', 'better', 'partiality', 'inconvenience', 'epidemic', 'modi', 'federal', 'still', 'ice', 'sentiments', 'maybe', 'half', 'agents', 'threats', 'amongst', 'milo', 'considered', 'dress', 'caps', 'italian', 'carefully', 'forces', 'pence', 'musk', 'strongly', 'prize', 'signs', 'acted', 'mcconnell', 'boasts', 'lab', 'song', 'sought', 'schiff', 'safe', 'we', 'congratulate', 'lately', 'looks', 'civility', 'smile', 'jong', 'arms', 'illness', 'department', 'watching', 'touts', '14', 'answers', 'busy', 'want', 'know', 'delightful', 'fund', 'decades', 'neighbours', 'drink', 'fond', 'alike', 'steps', 'despite', 'billion', 'advantage', 'ear', 'victim', 'sigh', 'might', 'reverse', 'along', 'children', 'regard', 'previously', 'hair', 'pressure', 'italy', 'macron', 'indignation', 'non', 'been', 'dressing', 'possibility', 'afterward', 'challenges', 'honour', 'calmness', 'answered', 'raillery', 'flag', 'lawyer', 'pretend', 'website', 'woman', 'lively', 'swept', 'assurance', 'hitler', 'fast', 'forgotten', 'linked', 'gate', 'passage', 'cop', 'announced', 'funeral', 'everybody', 'search', 'committed', 'maker', 'photo', 'texts', 'cherry', 'really', 'solitude', 'changing', 'backing', 'arrests', 'somersetshire', 'users', 'kellyanne', 'hereafter', '1', 'kept', 'shouted', 's', 'flower', 'straight', 'ate', 'cost', 'collected', 'disgusting', 'evidence', 'loose', 'monuments', 'mnuchin', 'laughed', 'prepared', 'difficulties', 'supported', 'declined', 'toll', 'above', 'survivor', 'crowd', 'private', 'interest', 'blessing', 'kill', 'trillion', 'possession', 'successful', 'spokesman', 'intention', 'prep', 'allow', 'talk', 'decries', 'knew', 'voice', 'airstrike', 'shed', 'oppose', 'permanent', 'equally', 'earnest', 'cart', 'dossier', 'past', 'information', 'flattered', 'deletes', 'nixon', 'some', 'hunt', 'prestigious', 'overturn', 'nsc', 'material', 'put', 'finding', 'guardian', 'friends', 'yemen', 'many', 'expect', 'line', 'directed', 'tillerson', 'party', 'spreads', 'spreading', 'multiple', 'rubio', 'gun', 'ten', 'begged', 'claim', 'mistaken', 'weakness', 'relieved', 'believing', 'ethnic', 'make', 'arrived', 'readiness', 'lord', 'grows', 'businesses', 'male', 'java', 'mark', 'thanks', 'concealed', 'instantly', 'colluded', 'consideration', 'brief', 'contract', 'coal', 'foreigners', 'direction', 'dwelling', 'influence', 'harassment', 'hand', 'made', 'orders', 'spend', 'lapierre', 'whose', 'shoot', 'choose', 'meddling', 'primary', 'pleasant', 'tricked', 'deserved', 'deportation', 'gathered', 'platform', 'proper', 'sports', 'wherever', 'amiable', 'joint', 'amends', 'rage', 'morton', 'vetoes', 'distance', 'bully', 'respond', 'practice', 'impatience', 'mistake', 'resort', 'throws', 'attacks', 'relationship', 'highway', 'daca', 'chicago', 'flew', 'ny', 'looms', 'migrants', 'introduced', 'things', 'rohingya', 'naturally', 'lowest', 'villagers', 'safely', 'wearing', 'harsh', 'jimmy', 'wasserman', 'id', 'babies', 'to', 'wall', 'she', 'habit', 'particulars', 'visitor', 'diamond', 'reach', 'county', 'create', 'anyone', 'person', 'asking', 'legalization', 'whale', 'puerto', 'john', 'reflection', 'live', 'front', 'satisfied', 'reply', 'tired', 'urge', 'hook', 'jump', 'suggest', 'khan', 'solar', 'tweets', 'least', 'peculiar', 'education', 'instance', 'denying', 'mental', 'bench', 'sex', 'guard', 'advisers', 'hundred', 'did', 'cnnpolitics.com', 'offering', 'wishes', 'taught', 'watch', 'tender', 'examine', 'rob', 'require', 'justify', 'ammunition', 'studies', 'what', 'fly', 'thanked', 'kathy', 'global', 'come', 'weed', 'arrival', 'cruelty', 'victory', 'immigration', 'impression', 'only', 'pulled', 'help', 'withdraw', 'longer', 'dept', 'chef', 'chiefly', 'crown', 'watergate', 'not', 'perceive', 'champion', 'active', 'gerrymandering', 'rival', 'loss', 'blessed', 'set', 'shyness', 'carter', 'poverty', 'utmost', 'spy', 'harvey', 'phony', 'shame', 'submit', 'hated', 'nice', 'withdrawing', 'doubt', 'already', 'gains', 'prosecutor', 'share', 'morrow', 'ties', 'ohio', 'says', 'confess', 'statement', 'delicious', 'congressman', 'won', 'hacking', 'equifax', 'replacing', 'reveal', 'affliction', 'world', 'fatigue', 'recovery', 'cut', 'kremlin', 'chapter', 'circuit', 'groups', 'equal', 'blasts', 'targeted', 'dead', 'foundation', 'afghan', 'rand', 'hypocrisy', 'offence', 'relatives', 'about', 'muslims', 'plane', 'charges', 'avoiding', 'bred', 'joked', 'frogs', 'production', 'alligator', 'spicer', 'majority', 'gay', 'kills', 'carolina', 'paper', 'troops', 'lessen', 're', 'gaining', 'unprecedented', 'anger', 'wo', 'privilege', 'me', 'garden', 'labour', 'defamation', 'jerusalem', 'eliza', 'condemnation', 'could', '7', 'scandals', 'stabbing', 'death', 'enemies', 'jailed', 'friday', 'aliens', 'whole', 'frank', 'remarks', 'diverted', 'nights', 'remove', 'middleton', 'thinks', 'and', 'questioning', 'bottom', 'if', 'tricks', 'rely', 'unfortunate', 'leisure', 'related', 'remains', 'announces', 'local', 'winter', 'security', 'yours', 'exact', 'lawmaker', 'investigates', 'master', 'suspended', 'larks', 'overhaul', 'agree', 'observed', 'sky', 'begging', 'edge', 'access', 'ferrars', 'offers', 'simple', 'mile', 'draft', 'grey', 'waited', 'esteem', 'wikileaks', 'stepping', 'servants', 'liberal', 'piece', 'turmoil', 'seated', 'impeached', 'lucky', 'states', 'immigrants', 'japan', 'improvements', 'clash', 'clintons', 'birth', 'rollaround', 'solid', 'founder', 'oil', 'gilded', 'info', 'weeds', 'absent', 'advocates', 'chaos', 'm', 'bolton', 'exchange', 'park', 'marriage', 'space', 'cage', 'while', 'bargain', 'moral', 'quick', 'service', 'leave', 'thinking', 'harm', 'slightly', 'announcing', 'listening', 'somebody', 'february', 'engage', 'courtiers', 'salad', 'weary', 'shady', 'sparking', 'memos', 'town', 'sharply', 'priebus', 'anxiety', 'files', 'elon', 'case', 'asian', 'grave', 'speaks', \"d'souza\", 'erupts', '2nd', 'reacts', 'koch', 'father', 'ruin', 'approval', 'mr.', 'drawing', 'reaches', 'earnestness', 'ring', 'switch', 'smiled', 'so', 'reasonably', 'neil', 'tucker', 'denmark', 'assist', 'charming', 'gracious', 'burned', 'at', 'c.', 'established', 'snap', 'books', 'conversations', 'proposal', 'democrat', 'wee', 'box', 'supposing', 'centers', 'shew', 'examining', 'stepped', 'never', 'partisan', 'citizenship', 'mining', 'hungry', 'raising', 'bent', 'documents', 'caution', 'lover', 'showed', 'flood', 'va', 'shall', 'obstruction', 'generous', 'beautifully', 'sunday', 'homes', 'consciousness', 'other', 'came', 'embarrassment', 'protest', 'worked', 'stories', 'opened', 'leak', 'swamp', 'air', 'suspected', 'rights', 'administration', 'mirth', 'tv', 'served', 'universal', 'lashes', 'sexually', 'confirm', 'food', 'torture', 'bipartisan', 'small', 'dining', 'cancels', 'parade', 'footage', 'snl', 'assange', 'melancholy', 'wait', 'depend', 'detention', 'importance', 'economy', 'hits', 'booker', 'team', 'onto', 'dinner', 'again', 'wake', 'any', 'properties', 'keep', 'drive', 'fixing', 'secure', 'german', 'crickets', 'price', 'serve', 'draws', 'millstones', 'level', 'tailor', 'jove', 'bowed', 'extend', 'tiger', 'matter', 'potential', 'absolutely', 'entreaties', 'cards', 'warming', 'sell', 'boat', 'foolish', 'energy', 'student', 'basket', 'astonished', '’s', 'number', 'its', 'uncomfortable', 'fever', 'river', 'massive', 'driving', 'her', 'or', 'farewell', 'grain', 'protester', 'look', 'apologizes', 'shortly', '       ', 'manafort', 'leaders', 'suliman', 'growth', 'undoubtedly', 'bracing', 'unhappiness', 'powell', 'northern', 'when', 'truth', 'head', 'wear', 'pointing', 'advice', 'luckily', 'twigs', 'withdraws', 'mueller', 'fails', 'publicly', 'rifles', 'social', 'sue', 'singapore', 'snowflakes', 'enabled', 'outside', 'urged', 'capable', 'entered', 'court', 'hands', 'keeping', 'event', 'gop', 'planned', 'censure', 'deceived', 'messing', 'trevor', 'payment', 'recent', 'goes', 'grass', 'franken', 'sensible', 'suppose', 'manchin', 'attentive', 'turn', 'persuade', 'needs', 'lets', 'revised', 'jet', 'word', 'promised', 'hero', '’re', 'scenes', 'myanmar', 'please', 'safety', 'requires', 'comfortable', 'tries', 'received', 'nearly', 'drama', 'questions', 'whisper', 'update', 'wit', 'alternatives', 'theories', 'row', 'memo', 'starbucks', 'aye', 'future', 'pledges', 'chuck', 'regime', 'car', 'genius', 'gain', 'prove', 'numbers', 'popularity', 'alternative', 'propaganda', 'policy', 'republicans', 'takeaways', 'cities', 'alleged', 'imposes', 'blaming', 'later', 'living', 'yet', 'success', 'cheerful', 'gets', 'sen.', 'reality', 'smith', 'called', 'appeared', 'bank', 'stocks', 'talked', 'behind', 'rabbit', 'birthplace', 'run', 'society', 'likes', 'metal', 'distress', 'than', 'others', 'rose', 'face', 'glass', 'african', 'evangelical', 'iraqi', 'antarctica', 'cold', 'finger', 'attempting', 'obliged', 'reportedly', 'sarah', 'touched', 'hell', 'nothing', 'agenda', 'accused', 'armies', 'using', 'starved', 'crash', 'opioid', 'zones', 'bob', 'budget', 'net', 'situation', 'pity', 'attended', 'jist', 'circumstance', 'wise', 'nodding', 'gulls', 'moments', 'seized', '24', 'idle', 'shining', 'bbc', 'slam', 'agitation', 'repeatedly', 'roy', 'embassy', 'members', 'ground', 'proof', 'tall', 'losing', 'bernstein', 'patience', 'employment', 'action', 'discretion', 'stage', 'tech', 'attachment', 'wealthy', 'islam', 'stephen', 'throat', 'cancer', 'storm', 'aclu', 'get', 'rest', 'tulip', 'flynn', 'decided', 'widow', 'catching', 'weak', 'bid', 'eagerly', 'fortune', 'fight', 'predicts', 'honor', 'exceedingly', 'harder', 'glad', 'tie', 'rejects', 'loans', 'ex', 'bin', 'blackberry', 'after', 'does', 'funny', 'oversight', 'parsonage', 'blue', 'arranged', 'right', 'build', 'who', 'fail', 'mounted', 'rendered', 'maintained', 'advise', 'soft', 'tortoise', 'electronics', 'said,--', 'beat', 'always', 'toss', 'rick', 'passed', 'names', 'exposed', 'uc', 'personally', 'vain', 'pulling', '7.9', 'provided', 'faces', 'baseball', 'button', 'required', 'mississippi', 'fury', 'allegations', 'trumps', 'cancel', 'barron', 'aim', 'cottage', 'chief', 'chose', 'privacy', 'corker', 'nightmare', 'giving', 'camel', 'common', 'restored', 'slash', 'calling', 'rising', 'conservative', 'kushners', 'married', 'cry', 'chile', 'expert', 'scramble', 'united', 'rolled', 'promotes', 'sank', 'scissors', 'annual', 'hath', 'day', 'committee', 'booming', 'saudis', 'england', 'discuss', 'humans', 'acknowledged', 'disappointed', 'berkeley', 'ted', 'overseeing', 'check', 'assertion', 'fought', 'tapes', 'anti', 'greatly', 'confederate', 'arise', 'epaminondas', 'employees', 'hurt', 'officer', 'laura', 'experts', '2019', 'admit', 'vast', 'determined', 'auntie', 'lgbt', 'attaching', 'stamps', 'inquiry', 'sets', 'principles', 'written', 'firmly', '2013', 'against', 'moore', 'lobbyist', 'relation', 'treatment', 'elinor', 'diplomatic', 'tried', 'oxford', 'preceding', 'heads', 'handle', 'breaks', 'keeps', 'refugees', 'worst', 'tax', 'bitterly', 'progress', 'wonderful', 'district', 'horse', 'investigator', 'deals', 'pro', 'was', 'reached', 'wisconsin', 'wolff', 'ratings', 'carriages', 'ideas', 'ought', 'funding', 'returns', 'stopped', 'thursday', 'fair', 'guessed', 'ambassador', 'inclination', 'openly', 'poor', 'anything', 'defy', 'satisfaction', 'newt', 'plymouth', 'themselves', 'survey', 'caucus', 'owner', 'mccarthy', 'g', 'honest', 'servant', 'praises', 'born', 'call', 'reporting', 'chuse', 'respect', 'thrown', 'even', 'instant', 'commendation', '3', '18', 'fusion', 'employments', 'chaffetz', 'syllable', 'failed', 'hin', 'healthcare', 'particular', 'women', 'accidentally', 'can', 'kanye', 'cease', 'teacher', 'vision', 'immediate', 'fears', 'pay', 'ago', 'imagined', 'promote', 'necessarily', 'salt', 'silent', 'regretting', 'youth', 'jack', 'claiming', 'allies', 'introduces', 'otter', 'radical', 'awful', 'rules', 'going', 'sharp', 'weinstein', 'carriage', 'hurry', 'trolls', 'danger', 'kingdom', 'return', 'sympathy', 'marry', 'army', 'nationalism', 'factory', 'launch', 'presidential', 'puzder', 'reverses', 'night', 'lazy', 'silence', 'changes', 'majesty', 'finally', 'which', 'mosul', 'lifts', 'mother', 'investigated', 'impeachment', 'arguments', 'acid', 'faith', 'succeeded', 'covered', 'smiling', 'saying', 'station', 'unveils', 'sweep', 'your', 'address', 'kind', 'another', 'gulf', 'almost', 'missing', 'forever', 'ally', 'daniels', 'germany', 'illegal', 'sacrifice', 'rips', 'bureau', 'korean', 'telling', 'say', 'ladies', 'welcome', 'begun', 'emergency', 'jackal', 'replaced', 'sharing', 'entrance', 'birthday', 'barn', 'happen', 'north', 'jealous', 'kids', 'now', 'filled', 'summoned', 'amp', 'site', 'beggars', 'seeming', 'friendship', 'inequality', 'terrorist', 'a', 'lovely', 'uncertainty', 'arrangement', 'wanting', 'train', 'correspondents', \"ma'am\", 'stupid', 'defeat', 'minority', 'save', 'convinced', 'syrian', 'is', 'unaccountable', 'behaviour', 'harris', 'monstrous', 'character', 'contempt', 'crabs', 'murders', 'general', 'during', 'protect', 'cast', 'organization', 'conduit', 'egypt', 'farther', 'sanctuary', 'florida', 'twelvemonth', 'expectation', 'greatest', 'sweden', 'moves', 'weigh', 'bush', 'has', 'protecting', 'grand', 'myself', 'interested', 'dared', 'erdogan', 'bitcoin', 'helped', 'notice', 'angry', 'eye', 'toward', 'place', 'stake', 'until', 'mexico', 'reveals', 'give', 'heart', 'complained', 'thousands', 'concert', 'home', 'debate', 'marrying', 'ho', 'single', 'alone', 'mexican', 'joins', 'feeling', 'therefore', 'witness', 'excellent', 'apparently', 'support', 'authorities', 'extremist', 'faithful', 'powers', 'athletes', 'unacceptable', 'adviser', 'missiles', 'ceo', 'door', 'kerry', 'ban', 'threatening', 'digital', 'arrives', 'elect', 'madrid', 'pairs', 'wholly', 'hillary', 'first', 'backward', 'dismissed', 'liked', 'pull', 'close', 'taxpayers', 'fortnight', 'image', 'pushed', 'disastrous', 'blows', 'ethics', 'lawsuit', 'receive', 'sincere', 'released', 'sounds', 'planted', 'reproach', 'risk', 'hiding', 'votes', 'betting', 'voluntarily', 'ok', 'hung', 'together', 'informed', 'mistress', 'assisted', 'push', 'diplomat', 'delivers', 'teens', 'quitted', 'uneasiness', 'learnt', 'unexpected', 'clown', 'spark', 'dies', 'high', 'unusual', 'club', 'tell', 'walking', 'authority', 'warrant', 'flowers', 'timeline', 'curbs', 'carry', 'twisted', 'attempts', 'jr.', 'minister', 'roared', 'pains', 'symptom', 'amnesty', 'manner', 'reporter', 'prick', 'dublin', 'wages', 'depended', 'embarrassing', 'league', 'releases', 'historic', 'coloured', 'clashes', 'abilities', 'speedily', 'progressive', 'supremacist', 'texas', 'convince', 'elephant', 'nikki', 'sorrow', 'command', 'dependence', 'manchester', 'records', 'uncertain', 'provide', 'follows', 'confession', 'confirmed', 'reports', 'challenge', 'being', 'receiving', 'hearing', 'dance', 'illegally', 'words', 'pipeline', 'clears', 'marianne', 'happy', 'much', 'raised', 'fed', 'voices', 'stayed', 'boy', 'messenger', 'tests', 'story', 'demand', 'apparent', 'wrap', 'willing', 'xiaobo', 'subject', 'company', 'tesla', 'pot', 'reich', 'suspicion', 'minutes', 'deep', 'serious', 'widen', 'leaves', 'luck', 'promises', 'staffer', 'judiciary', 'comedy', 'admits', 'strange', 'lawmakers', 'guest', 'nunes', 'acquainted', 'denied', 'nasa', 'rises', 'match', 'said', 'hastily', 'slower', 'watchdog', 'him', 'i', 'beto', 'why', 'leather', 'proposed', 'done', 'sprang', 'ed', 'sure', 'whatever', 'grow', 'style', 'sandy', 'francis', 'sanctions', 'golden', 'legacy', 'opinion', 'flights', \"o'clock\", 'scores', 'suspense', 'fault', 'efforts', 'accident', 'evening', 'told', 'whom', 'spending', 'key', 'judicial', 'louise', 'urgent', 'prevail', 'center', 'beyond', 'standing', 'bond', 'have', 'agreed', 'bilateral', 'beds', 'wretched', 'lessons', 'nyc', 'spared', 'task', 'commission', 'deny', 'sheriff', 'satisfactory', 'europe', 'tariffs', 'ross', 'limits', 'resigning', 'clean', 'burst', 'tougher', 'free', 'theory', 'activist', 'claims', 'disappearing', 'catch', 'takes', 'situations', '9', 'legal', 'conviction', 'bias', 'hints', 'due', 'discovered', 'sadly', 'he', 'austrian', 'understanding', 'weaken', 'manufactured', 'design', 'islamic', 'tree', 'stood', 'remembered', 'affections', 'revives', 'dumber', 'le', 'opts', 'aluminum', 'ran', 'ah', 'seen', 'surge', 'rejected', 'saudi', 'exactly', 'nurse', 'will', 'statue', 'film', 'getting', 'pink', 'tapper', 'hacked', 'posts', 'warnings', 'protests', 'offensive', 'unlike', 'newspaper', 'jennings', 'objection', 'peace', 'investigate', 'keystone', 'rough', 'taliban', 'low', 'fuel', 'rise', 'son', 'rating', 'sues', 'anybody', 'messages', 'closely', 'harp', 'battle', 'russia', 'shone', 'base', 'coverage', 'admitted', 'assistance', 'bestowed', 'pollution', 'proceeded', 'encouraged', 'steal', 'easy', 'finds', 'democracy', 'conscious', 'pundits', 'lasted', 'wounded', 'somehow', 'procured', 'exeter', 'zero', 'shares', 'hire', 'powerful', 'favourite', 'figs', 'returning', 'fancy', 'fix', 'mama', 'allowed', 'pardon', 'filibuster', 'revealed', 'board', 'roger', 'becomes', 'preview', 'expenses', 'jumped', 'before', 'sincerely', 'admiration', 'invariably', 'departure', 'scene', 'lady', 'lies', 'saul', 'internet', 'rep.', 'floor', 'removal', 'hill', 'astonishment', 'israel', 'flip', 'laugh', 'mine', 'land', 'choked', 'knife', 'meme', 'below', 'nbc', 'whip', 'nato', 'involved', 'mainstream', 'bowl', 'us', 'individual', 'cpac', 'soon', 'liberty', 'housing', 'wide', 'furor', 'ambitions', 'something', 'sold', 'yourself', 'supreme', 'message', 'france', 'instrument', 'markets', 'citing', 'midterms', 'wave', 'angeles', 'kong', 'diplomacy', 'kid', 'staff', 'inclined', '$', 'rid', 'preference', 'nazi', 'subjects', 'nobody', 'hers', 'surprised', 'ca', 'exert', 'sabotage', 'gaza', 'six', 'freedom', 'breath', 'persuasion', 'replied', 'concerns', 'carriers', 'retaliation', 'beneath', 'lights', 'vow', 'jpmorgan', 'fake', 'alt', 'sang', 'seeks', 'troll', 'wid', 'sentence', 'in', 'alabama', 'zuckerberg', 'shot', 'licenses', 'vulnerable', 'hosts', 'lands', 'difference', 'fear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlK2KhZCbqg2"
      },
      "source": [
        "OOV_token = \"<OOV>\"\n",
        "vocabulary.add(OOV_token)\n",
        "word2idx = {}\n",
        "n_words = 0\n",
        "\n",
        "tokenized_corpus_with_OOV = []\n",
        "for sentence in tokenized_corpus:\n",
        "\n",
        "    tokenized_sentence_with_OOV = []\n",
        "    for token in sentence:\n",
        "        if token in vocabulary:\n",
        "            tokenized_sentence_with_OOV.append(token)\n",
        "        else:\n",
        "            tokenized_sentence_with_OOV.append(OOV_token)\n",
        "    tokenized_corpus_with_OOV.append(tokenized_sentence_with_OOV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-lO4VQbuTT"
      },
      "source": [
        "for token in vocabulary:\n",
        "    if token not in word2idx:\n",
        "        word2idx[token] = n_words\n",
        "        n_words += 1\n",
        "\n",
        "assert len(word2idx) == len(vocabulary)\n",
        "\n",
        "# Invert dictionary\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "assert len(idx2word) == len(word2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru0npEpjftsw"
      },
      "source": [
        "def get_focus_context_pairs(tokenized_corpus, window_size=2):\n",
        "    focus_context_pairs = []\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token_idx, token in enumerate(sentence):\n",
        "            for w in range(-window_size, window_size+1):\n",
        "                context_word_pos = token_idx + w\n",
        "\n",
        "                if w == 0 or context_word_pos >= len(sentence) or context_word_pos < 0:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    focus_context_pairs.append([token, sentence[context_word_pos]])\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    return focus_context_pairs\n",
        "focus_context_pairs = get_focus_context_pairs(tokenized_corpus_with_OOV)\n",
        "\n",
        "print(focus_context_pairs[0:40])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7GLEYh9f4gs",
        "outputId": "c9cb1a12-d0ef-45fc-d0ce-257660afc0c7"
      },
      "source": [
        "def get_focus_context_idx(focus_context_pairs):\n",
        "    idx_pairs = []\n",
        "    for pair in focus_context_pairs:\n",
        "        idx_pairs.append([word2idx[pair[0]], word2idx[pair[1]]])\n",
        "    \n",
        "    return idx_pairs\n",
        "\n",
        "\n",
        "idx_pairs = get_focus_context_idx(focus_context_pairs)\n",
        "\n",
        "print(idx_pairs[0:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1926, 3469], [1926, 1808], [3469, 1926], [3469, 1808], [3469, 816], [1808, 1926], [1808, 3469], [1808, 816], [1808, 1258], [816, 3469], [816, 1808], [816, 1258], [816, 4533], [1258, 1808], [1258, 816], [1258, 4533], [1258, 4533], [4533, 816], [4533, 1258], [4533, 4533]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqkMlTEFgMMN"
      },
      "source": [
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Why do you think we don't have an activation function here?\n",
        "        self.projection = nn.Linear(input_size, hidden_dim_size, bias=False)\n",
        "        self.output = nn.Linear(hidden_dim_size, output_size)\n",
        "        \n",
        "    def forward(self, input_token):\n",
        "        x = self.projection(input_token)\n",
        "        output = self.output(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPyia5UDg3aQ"
      },
      "source": [
        "def get_one_hot(indicies, vocab_size=len(vocabulary)):\n",
        "    oh_matrix = np.zeros((len(indicies), vocab_size))\n",
        "    for i, idx in enumerate(indicies):\n",
        "        oh_matrix[i, idx] = 1\n",
        "\n",
        "    return torch.Tensor(oh_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLJxtFH-gTP4"
      },
      "source": [
        "def train(word2vec_model, idx_pairs, state_dict_filename, early_stop=False, num_epochs=10, lr=1e-3):\n",
        "\n",
        "    word2vec_model.train()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(word2vec_model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        random.shuffle(idx_pairs)\n",
        "\n",
        "        for focus, context in idx_pairs:\n",
        "            print(focus)\n",
        "            oh_inputs = get_one_hot([focus], len(vocabulary))\n",
        "            target = torch.LongTensor([context])\n",
        "\n",
        "            pred_outputs = word2vec_model(oh_inputs)\n",
        "\n",
        "            loss = criterion(pred_outputs, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            word2vec_model.zero_grad()\n",
        "            \n",
        "        ### These lines stop training early\n",
        "            if early_stop: break\n",
        "        if early_stop: break\n",
        "        ###\n",
        "\n",
        "        torch.save(word2vec_model.state_dict(), state_dict_filename)\n",
        "        writer.add_embedding(word2vec_model.projection.weight.T,\n",
        "                             metadata=word2idx.keys(), global_step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9jHZIQNgXsd"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('runs/word2vec_gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihqPAQM6ghsd",
        "outputId": "f08a8167-830c-4d44-bb21-4030e88bd1ab"
      },
      "source": [
        "w2v_gutenberg = Word2Vec(len(vocabulary), len(vocabulary), 128)\n",
        "train(w2v_gutenberg, idx_pairs, \"word2vec_gutenberg.pt\", early_stop=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO1p9Hj5gqda",
        "outputId": "8da0d186-725a-4e56-b9f1-becc547ad842"
      },
      "source": [
        "weights_matrix = w2v_gutenberg.projection.weight.T\n",
        "print(weights_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4738, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udJACzxtK1TG"
      },
      "source": [
        "## Predictions based on cosine distance between replacement words and edit words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3Z7mQgJMw2a"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHvjs44AK_uh"
      },
      "source": [
        "#get idx for old word\n",
        "old_idx = []\n",
        "for word in train_df.old.str.lower():\n",
        "    #that also excludes words that consist of two words. not good\n",
        "    if word in list(word2idx.keys()):\n",
        "        old_idx.append(word2idx[word])\n",
        "    else:\n",
        "        word = OOV_token\n",
        "        old_idx.append(word2idx[word])\n",
        "\n",
        "#get idx for edit word\n",
        "edit_idx = []\n",
        "for word in train_df.edit.str.lower():\n",
        "    #that also excludes words that consist of two words. not good\n",
        "    if word in list(word2idx.keys()):\n",
        "        edit_idx.append(word2idx[word])\n",
        "    else:\n",
        "        word = OOV_token\n",
        "        edit_idx.append(word2idx[word])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWIQq9JMLCpy"
      },
      "source": [
        "old_word_embedding = get_one_hot(old_idx) @ weights_matrix.detach().numpy()\n",
        "edit_embedding = get_one_hot(edit_idx) @ weights_matrix.detach().numpy()\n",
        "\n",
        "\n",
        "distances = []\n",
        "for i in range(len(old_word_embedding)):\n",
        "    cos_distance = cosine(old_word_embedding[i], edit_embedding[i])\n",
        "    distances.append(cos_distance)\n",
        "train_df['cos_distance'] = distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzdrWQLHLGkW"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBHnIFV4LKRy"
      },
      "source": [
        "plt.scatter(train_df.cos_distance.to_list(), train_df.meanGrade.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MukQ7JqJLNKw"
      },
      "source": [
        "train_df.loc[train_df.cos_distance == 0.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgHJ5bt6LP1K"
      },
      "source": [
        "indexNames = train_df[(train_df['cos_distance'] == 0.0)].index\n",
        "# Delete these row indexes from dataFrame\n",
        "train_df.drop(indexNames , inplace=True)\n",
        "train_df.reset_index(inplace=True,drop=True)\n",
        "pd.set_option('display.max_rows', 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woJbN9h_LSl3"
      },
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "model = linear_model.LinearRegression()\n",
        "train, test = train_test_split(train_df, test_size=0.2)\n",
        "dist_train = np.reshape(train.cos_distance.to_list(), (-1,1))\n",
        "dist_test = np.reshape(test.cos_distance.to_list(), (-1,1))\n",
        "y = np.reshape(train.meanGrade.to_list(), (-1,1))\n",
        "y_test = np.reshape(test.meanGrade.to_list(), (-1,1))\n",
        "#print(train.cos_distance)\n",
        "\n",
        "model.fit(dist_train, y)\n",
        "predictions = model.predict(dist_test)\n",
        "\n",
        "mean_squared_error(y_test, predictions, squared = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7jxXc6uLU6F"
      },
      "source": [
        "plt.scatter(predictions,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYSdwJ1XLVia"
      },
      "source": [
        "## Baseline given\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK9C7EeEaRLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596ed31b-d4c4-47db-bd40-739276dbbd7e"
      },
      "source": [
        "train_and_dev = train_df['edit']\n",
        "train_proportion = 0.8\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "transformer\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "print(train_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 6912)\t0.3196422657133493\n",
            "  (0, 6889)\t0.4635074138381324\n",
            "  (0, 6423)\t0.11278822224285272\n",
            "  (0, 5804)\t0.44285750586411565\n",
            "  (0, 5374)\t0.23652438242454743\n",
            "  (0, 5312)\t0.3400955840429778\n",
            "  (0, 4309)\t0.34740743437045796\n",
            "  (0, 4089)\t0.2845496729305107\n",
            "  (0, 3299)\t0.31761063288602626\n",
            "  (1, 6541)\t0.48194793383580115\n",
            "  (1, 5549)\t0.3204233579379913\n",
            "  (1, 5014)\t0.42370065991381706\n",
            "  (1, 2759)\t0.4595615370435467\n",
            "  (1, 2464)\t0.5237709472831734\n",
            "  (2, 6892)\t0.35794538379097757\n",
            "  (2, 6156)\t0.36483747544305006\n",
            "  (2, 5741)\t0.3829658592421385\n",
            "  (2, 5499)\t0.3419984129026347\n",
            "  (2, 4801)\t0.3829658592421385\n",
            "  (2, 4305)\t0.291413316187634\n",
            "  (2, 3322)\t0.313301064206565\n",
            "  (2, 2076)\t0.3110077361041403\n",
            "  (2, 1141)\t0.2206241510260133\n",
            "  (3, 6534)\t0.39337308644812724\n",
            "  (3, 6423)\t0.09734562388426601\n",
            "  :\t:\n",
            "  (7717, 6019)\t0.520234681094425\n",
            "  (7717, 5375)\t0.28639284053699743\n",
            "  (7717, 3451)\t0.4360369675276321\n",
            "  (7717, 3299)\t0.3331918391659665\n",
            "  (7717, 2782)\t0.46458301954981385\n",
            "  (7717, 2533)\t0.3610437600682453\n",
            "  (7718, 4700)\t0.400658806873244\n",
            "  (7718, 4635)\t0.5126970556930236\n",
            "  (7718, 3590)\t0.3448996293605981\n",
            "  (7718, 3569)\t0.35473412275396105\n",
            "  (7718, 3532)\t0.3569109378132053\n",
            "  (7718, 1359)\t0.4521468565066124\n",
            "  (7719, 4980)\t0.4205214064768076\n",
            "  (7719, 3410)\t0.3027977180838381\n",
            "  (7719, 3409)\t0.3930472980843201\n",
            "  (7719, 3372)\t0.3930472980843201\n",
            "  (7719, 212)\t0.40061526270871783\n",
            "  (7719, 56)\t0.4205214064768076\n",
            "  (7719, 0)\t0.2918425766313267\n",
            "  (7720, 6843)\t0.513093481562848\n",
            "  (7720, 6031)\t0.4297411912433994\n",
            "  (7720, 4854)\t0.3381402403225188\n",
            "  (7720, 3981)\t0.32744999099295835\n",
            "  (7720, 3347)\t0.4239023691800264\n",
            "  (7720, 3322)\t0.38833239679856807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyJFzk7aEwcd",
        "outputId": "574ac346-d713-48ef-e732-171ce4c3ecbe"
      },
      "source": [
        "mean = train_df['meanGrade'].mean()\n",
        "sum = 0\n",
        "for i in range(len(train_df['meanGrade'])):\n",
        "    sum += (mean - train_df['meanGrade'].loc[i])**2\n",
        "np.sqrt(sum / len(train_df['meanGrade']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5836128526966657"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AInKATQT6pXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b2b52a-2533-466d-dd35-2cea9b93e977"
      },
      "source": [
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse, _ = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse, _ = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train performance:\n",
            "| MSE: 0.16 | RMSE: 0.40 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.48 | RMSE: 0.69 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lRZxL-gD40S"
      },
      "source": [
        "'''\n",
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedded = self.embedding(sentence)\n",
        "        embedded = embedded.permute(1, 0, 2) #Needed for LSTMs\n",
        "\n",
        "        #lstm_out : (seq_len,batch_size,num_directions (2) * hidden_size (50))\n",
        "        #hidden : (num_layers * num_directions, batch_size,hidden_size)\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
        "\n",
        "        #out : (1)\n",
        "        out = self.hidden2label(lstm_out[-1]\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSZbSxUbaRLc"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwwE7oj0aRLd"
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vs5_tGhaRLd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}