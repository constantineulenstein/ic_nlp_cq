{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task_1_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNppdFiGaRLE"
      },
      "source": [
        "### Coursework coding instructions (please also see full coursework spec)\n",
        "\n",
        "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
        "\n",
        "For the task you choose you will need to do two approaches:\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "We should be able to run both approaches from the same colab file\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "\n",
        "#### Reproducibility:\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "Good luck! We are really looking forward to seeing your reports and your model code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8IanU_MaRLH"
      },
      "source": [
        "#Todo\n",
        "#Remove punctuation\n",
        "#Use embedding that represents headslines\n",
        "#Tokenizer with special token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WfDREnZaRLI",
        "outputId": "85c66385-aa12-428c-93d2-f88eee690c26"
      },
      "source": [
        "# You will need to download any word embeddings required for your code, e.g.:\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
        "\n",
        "#! pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-11 14:42:12--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-11 14:42:12--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-11 14:42:13--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 51s  \n",
            "\n",
            "2021-02-11 14:49:04 (2.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYZiUqXFghTY",
        "outputId": "332f60f2-7c43-4847-8920-b066e9a7b5ab"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFcC8gvDaRLI"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs\n",
        "\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import re\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84OyAVNSaRLJ",
        "outputId": "ef4fce60-0dfe-44aa-82f3-2a2997a6ea95"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2gxbblziaRLK",
        "outputId": "f1421511-2aee-4482-d5c0-58f1b1032db9"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('./drive/MyDrive/Imperial_College/NLP_CW/ic_nlp_cw/task-1/train.csv')\n",
        "test_df = pd.read_csv('./drive/MyDrive/Imperial_College/NLP_CW/ic_nlp_cw/task-1/dev.csv')\n",
        "train_df"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original</th>\n",
              "      <th>edit</th>\n",
              "      <th>grades</th>\n",
              "      <th>meanGrade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14530</td>\n",
              "      <td>France is ‘ hunting down its citizens who join...</td>\n",
              "      <td>twins</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13034</td>\n",
              "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
              "      <td>bowling</td>\n",
              "      <td>33110</td>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8731</td>\n",
              "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
              "      <td>party</td>\n",
              "      <td>22100</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>76</td>\n",
              "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
              "      <td>slap</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6164</td>\n",
              "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
              "      <td>school</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9647</th>\n",
              "      <td>10899</td>\n",
              "      <td>State officials blast ' unprecedented ' DHS &lt;m...</td>\n",
              "      <td>idea</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9648</th>\n",
              "      <td>1781</td>\n",
              "      <td>Protesters Rally for &lt;Refugees/&gt; Detained at J...</td>\n",
              "      <td>stewardesses</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9649</th>\n",
              "      <td>5628</td>\n",
              "      <td>Cruise line Carnival Corp. joins the fight aga...</td>\n",
              "      <td>raisin</td>\n",
              "      <td>21000</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9650</th>\n",
              "      <td>14483</td>\n",
              "      <td>Columbia police hunt woman seen with &lt;gun/&gt; ne...</td>\n",
              "      <td>cake</td>\n",
              "      <td>32200</td>\n",
              "      <td>1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9651</th>\n",
              "      <td>5255</td>\n",
              "      <td>Here 's What 's In The House-Approved Health &lt;...</td>\n",
              "      <td>food</td>\n",
              "      <td>11000</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9652 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... meanGrade\n",
              "0     14530  ...       0.2\n",
              "1     13034  ...       1.6\n",
              "2      8731  ...       1.0\n",
              "3        76  ...       0.4\n",
              "4      6164  ...       0.0\n",
              "...     ...  ...       ...\n",
              "9647  10899  ...       0.0\n",
              "9648   1781  ...       0.4\n",
              "9649   5628  ...       0.6\n",
              "9650  14483  ...       1.4\n",
              "9651   5255  ...       0.4\n",
              "\n",
              "[9652 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQbm--SyaRLL"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDfCSZPUHSQL"
      },
      "source": [
        "x = np.ones((32,25))\n",
        "y = np.ones((32,25))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxhwWCz6aRLL"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    train_losses = np.zeros(number_epoch)\n",
        "    valid_losses = np.zeros(number_epoch)\n",
        "    print(\"Training model.\")\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            feature, target = batch\n",
        "            feature_1= feature[0].to(device)\n",
        "            feature_2 = feature[1].to(device)\n",
        "            target = target.to(device)\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden_1 = model.init_hidden()\n",
        "            model.hidden_2 = model.init_hidden()\n",
        "\n",
        "            feature = (feature_1, feature_2)\n",
        "            predictions = model(feature).squeeze(1)\n",
        "\n",
        "            #print(predictions.shape)\n",
        "            #print(target.shape)\n",
        "\n",
        "            loss = loss_fn(predictions, target)\n",
        "                        \n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy()) \n",
        "            \n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        train_losses[epoch-1] = epoch_loss\n",
        "        valid_losses[epoch-1] = valid_loss\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')\n",
        "    return train_losses, valid_losses"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAPacdgOaRLM"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "            feature_1= feature[0].to(device)\n",
        "            feature_2 = feature[1].to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            feature = (feature_1, feature_2)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden_1 = model.init_hidden()\n",
        "            model.hidden_2 = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            #predictions.requires_grad = True\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcchQKtEaRLN"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO4Kv0URaRLN"
      },
      "source": [
        "def create_vocab(data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "\n",
        "            tokenized_sentence.append(token)\n",
        "\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSQ-36SdaRLO"
      },
      "source": [
        "# To create our vocab\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    \n",
        "    replacement_re = re.compile(r'^<*/>') #do not split replacement format\n",
        "    prefix_re = re.compile(r'''^[\\[\\(\"]''')\n",
        "    suffix_re = re.compile(r''',[\\]\\)\"']$''')\n",
        "    infix_re = re.compile(r'''[-\\,.~]''')\n",
        "    \n",
        "    \n",
        "    return Tokenizer(nlp.vocab,\n",
        "                     token_match = replacement_re.match,\n",
        "                     prefix_search=prefix_re.search,\n",
        "                     suffix_search=suffix_re.search,\n",
        "                     infix_finditer = infix_re.finditer  \n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "def preprocessor(data,edits):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    punctuation = \"\\\":\\.,\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    nlp.tokenizer = custom_tokenizer(nlp)\n",
        "    \n",
        "    tokenized_corpus_1= [] # Let us put the tokenized corpus in a list\n",
        "    tokenized_corpus_2= []\n",
        "    \n",
        "    \n",
        "    for i, sentence in enumerate(data):\n",
        "\n",
        "        sentence = sentence.lower()\n",
        "        tokenized_sentence_1 = []\n",
        "        tokenized_sentence_2 = []\n",
        "\n",
        "        for token in nlp(sentence): # simplest split is\n",
        "\n",
        "            if token.text in punctuation:\n",
        "                continue\n",
        "            else:\n",
        "                \n",
        "                if token.text[0] == '<':\n",
        "                    tokenized_sentence_1.append(\"???\")\n",
        "                    tokenized_sentence_2.append(\"???\")\n",
        "                    \n",
        "                    tokenized_sentence_2.append(edits[i])\n",
        "\n",
        "                    \n",
        "                    tokenized_sentence_1.append(token.text[1:-2])\n",
        "                else:\n",
        "                    tokenized_sentence_2.append(token.text)\n",
        "                \n",
        "                    tokenized_sentence_1.append(token.text)\n",
        "\n",
        "\n",
        "        tokenized_corpus_1.append(tokenized_sentence_1)\n",
        "        tokenized_corpus_2.append(tokenized_sentence_2)\n",
        "        #print(tokenized_corpus_1[:5])\n",
        "        #print(tokenized_corpus_2[:5])\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus_1:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "    \n",
        "    for token in edits:\n",
        "        \n",
        "        if token not in vocabulary:\n",
        "            \n",
        "            vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus_1, tokenized_corpus_2"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOOe3Ca7aRLP"
      },
      "source": [
        "'''def collate_fn_padd(batch):\n",
        "    #We add padding to our minibatches and create tensors for our model\n",
        "\n",
        "\n",
        "    batch_labels = [l for f, l in batch]\n",
        "    batch_features = [f for f, l in batch]\n",
        "\n",
        "    batch_features_len = [len(f) for f, l in batch]\n",
        "\n",
        "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\n",
        "\n",
        "    return seq_tensor, batch_labels\n",
        "'''\n",
        "def collate_fn_padd(batch):\n",
        "    \n",
        "    #We add padding to our minibatches and create tensors for our model\n",
        "    \n",
        "\n",
        "    batch_labels = [l for f, g, l in batch]\n",
        "    batch_features = [(f,g) for f, g, l in batch]\n",
        "    \n",
        "\n",
        "    batch_features_len = [len(f) for f, g, l in batch]\n",
        "\n",
        "\n",
        "    seq_tensor_1 = torch.zeros((len(batch), 50)).long()\n",
        "    seq_tensor_2 = torch.zeros((len(batch), 50)).long()\n",
        "\n",
        "    \n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor_1[idx, :seqlen] = torch.LongTensor(seq[0])\n",
        "        seq_tensor_2[idx, :seqlen] = torch.LongTensor(seq[1])\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\n",
        "    \n",
        "\n",
        "    return (seq_tensor_1, seq_tensor_2), batch_labels\n",
        "\n",
        "'''\n",
        "def collate_fn_pad(batch):\n",
        "\n",
        "    original, edit, labels = zip(*batch)\n",
        "    padded_original = torch.nn.utils.rnn.pad_sequence(original, batch_first=True,padding_value=0)\n",
        "    padded_edit = torch.nn.utils.rnn.pad_sequence(edit, batch_first=True,padding_value=0)\n",
        "    labels = torch.Tensor(labels)\n",
        "    return (padded_org, padded_edit, labels)\n",
        "'''\n",
        "\n",
        "\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, train_data, labels):\n",
        "        self.x_train = train_data\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train[item], self.y_train[item]\n",
        "    \n",
        "class Task1Dataset_double(Dataset):\n",
        "\n",
        "    def __init__(self, train_data_1,train_data_2, labels):\n",
        "        self.x_train_1 = train_data_1\n",
        "        self.x_train_2 = train_data_2\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train_1[item],self.x_train_2[item], self.y_train[item]    \n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1alD5n1GaRLR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "6baf637a-7452-4ab6-af4d-0acd0f2a75b6"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedded = self.embedding(sentence)\n",
        "        embedded = embedded.permute(1, 0, 2) #Needed for LSTMs\n",
        "\n",
        "        #lstm_out : (seq_len,batch_size,num_directions (2) * hidden_size (50))\n",
        "        #hidden : (num_layers * num_directions, batch_size,hidden_size)\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
        "\n",
        "        #out : (1)\n",
        "        out = self.hidden2label(lstm_out[-1]\n",
        "        return out"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-8a1a32850b62>\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    return out\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F1kXohWaRLR"
      },
      "source": [
        "class BiLSTM_double(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(BiLSTM_double, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm_1 = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.lstm_2 = nn.LSTM(embedding_dim,hidden_dim, bidirectional = True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.fc = nn.Linear(2*50*100, hidden_dim*2)\n",
        "        self.hidden_1 = self.init_hidden()\n",
        "        self.hidden_2 = self.init_hidden()\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "\n",
        "        self.d1 = nn.Dropout(0.5)\n",
        "        self.d2 = nn.Dropout(0.5)\n",
        "        self.d3 = nn.Dropout(0.5)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        \n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim, requires_grad = True).to(self.device), \\\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim, requires_grad = True).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        self.embedded_1 = self.embedding(sentence[0])\n",
        "        self.embedded_1 = self.embedded_1.permute(1, 0, 2) #Needed for LSTMs\n",
        "\n",
        "        self.embedded_2 = self.embedding(sentence[1])\n",
        "        self.embedded_2 = self.embedded_2.permute(1, 0, 2) #Needed for LSTMs\n",
        "\n",
        "        #lstm_out : (seq_len,batch_size,num_directions (2) * hidden_size (50))\n",
        "        #hidden : (num_layers * num_directions, batch_size,hidden_size)\n",
        "        lstm_out_1, self.hidden_1 = self.lstm_1(\n",
        "            self.embedded_1.view(len(self.embedded_1), self.batch_size, self.embedding_dim), self.hidden_1)\n",
        "        \n",
        "        lstm_out_1 = F.leaky_relu(self.d1(lstm_out_1))\n",
        "\n",
        "        lstm_out_2, self.hidden_2 = self.lstm_2(\n",
        "            self.embedded_2.view(len(self.embedded_2), self.batch_size, self.embedding_dim), self.hidden_2)\n",
        "        \n",
        "        lstm_out_2 = F.leaky_relu(self.d2(lstm_out_2))\n",
        "        \n",
        "        #out : (1)\n",
        "        lstm_out_1 = lstm_out_1.permute(1,0,2)\n",
        "        lstm_out_2 = lstm_out_2.permute(1,0,2)\n",
        "\n",
        "        out1 = self.fc(torch.cat((lstm_out_1.reshape(self.batch_size, -1),lstm_out_2.reshape(self.batch_size, -1)), dim = 1))\n",
        "        out1 = F.leaky_relu(self.d3(out1))\n",
        "\n",
        "        out = self.hidden2label(out1)\n",
        "        return out"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JSWm-Q5nIxX",
        "outputId": "779db43e-05e2-4d08-cd14-ec7ae5104c29"
      },
      "source": [
        "## Approach 1 code, using functions defined above:\n",
        "\n",
        "# We set our training data and test data\n",
        "training_data = train_df['original']\n",
        "test_data = test_df['original']\n",
        "\n",
        "# Creating word vectors\n",
        "#training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
        "#test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
        "training_vocab, training_tokenized_corpus_1,training_tokenized_corpus_2=  preprocessor(training_data,train_df['edit'])\n",
        "test_vocab, test_tokenized_corpus_1,test_tokenized_corpus_2=  preprocessor(test_data,test_df['edit'])\n",
        "\n",
        "print(\"Vocabulary individual creation - done\")\n",
        "\n",
        "# Creating joint vocab from test and train:\n",
        "#joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\n",
        "joint_vocab, joint_tokenized_corpus_1,joint_tokenized_corpus_2 = preprocessor(pd.concat([training_data, test_data]),pd.concat([train_df['edit'],test_df['edit']],ignore_index = True))\n",
        "\n",
        "print(\"Vocabulary joined creation - done\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Vocab created.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary individual creation - done\n",
            "Vocabulary joined creation - done\n",
            "Vocab created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeXy7v7dnQGq"
      },
      "source": [
        "# We create representations for our tokens\n",
        "wvecs = [] # word vectors\n",
        "word2idx = [] # word2index\n",
        "idx2word = []\n",
        "\n",
        "#Add special character -> embedding vector of ones \n",
        "wvecs.append(np.ones(100))\n",
        "\n",
        "# This is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f:\n",
        "  index = 1 #zero padding\n",
        "  for line in f.readlines():\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in joint_vocab:\n",
        "          (word, vec) = (word,\n",
        "                     list(map(float,line.strip().split()[1:])))\n",
        "          wvecs.append(vec)\n",
        "          word2idx.append((word, index))\n",
        "          idx2word.append((index, word))\n",
        "          index += 1\n",
        "\n",
        "wvecs = np.array(wvecs)\n",
        "word2idx = dict(word2idx)\n",
        "idx2word = dict(idx2word)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ze3glIrVMmj"
      },
      "source": [
        "word2idx['<unk>'] = 1\n",
        "idx2word[1] = '<unk>'\n",
        "mean = np.mean(wvecs, axis=0) # initialize unknown token as mean\n",
        "#wvecs = np.vstack((wvecs, mean))\n",
        "wvecs[0] = mean\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgcGpchbQOO-"
      },
      "source": [
        "\n",
        "\n",
        "vectorized_seqs_1 = [[word2idx[tok] if tok in word2idx else word2idx['<unk>'] for tok in seq] for seq in training_tokenized_corpus_1]\n",
        "vectorized_seqs_2 = [[word2idx[tok] if tok in word2idx else word2idx['<unk>'] for tok in seq] for seq in training_tokenized_corpus_2]\n",
        "\n",
        "\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\n",
        "vectorized_seqs_1 = [x if len(x) > 0 else [0] for x in vectorized_seqs_1]\n",
        "vectorized_seqs_2 = [x if len(x) > 0 else [0] for x in vectorized_seqs_2]\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XzJHcQ_aRLT",
        "scrolled": true,
        "outputId": "0aa16f80-4388-45eb-fd5b-639df51199ef"
      },
      "source": [
        "\n",
        "INPUT_DIM = len(word2idx)\n",
        "EMBEDDING_DIM = wvecs.shape[1]\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "model = BiLSTM_double(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(params)\n",
        "#print(\"Total number of parameters is: {​​}​​\".format(params))\n",
        "\n",
        "print(model)\n",
        "\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model.to(device)\n",
        "# We provide the model with our embeddings\n",
        "#x = np.concatenate((wvecs,wvecs),axis=1)\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2252201\n",
            "BiLSTM_double(\n",
            "  (embedding): Embedding(11304, 100, padding_idx=0)\n",
            "  (lstm_1): LSTM(100, 50, bidirectional=True)\n",
            "  (lstm_2): LSTM(100, 50, bidirectional=True)\n",
            "  (fc): Linear(in_features=10000, out_features=100, bias=True)\n",
            "  (hidden2label): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (d1): Dropout(p=0.5, inplace=False)\n",
            "  (d2): Dropout(p=0.5, inplace=False)\n",
            "  (d3): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Model initialised.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-7.8262e-04,  1.0269e-01,  1.4887e-01,  ..., -6.1605e-02,\n",
              "          2.0113e-01,  8.2360e-02],\n",
              "        [-3.8194e-02, -2.4487e-01,  7.2812e-01,  ..., -1.4590e-01,\n",
              "          8.2780e-01,  2.7062e-01],\n",
              "        [-1.5290e-01, -2.4279e-01,  8.9837e-01,  ..., -5.9100e-01,\n",
              "          1.0039e+00,  2.0664e-01],\n",
              "        ...,\n",
              "        [ 1.9771e-01, -6.8821e-02,  1.9041e-02,  ...,  1.3000e-01,\n",
              "         -2.7314e-01, -4.0290e-02],\n",
              "        [ 1.2610e-01, -2.7248e-01, -3.9575e-01,  ...,  4.6913e-01,\n",
              "          2.1689e-02, -6.4294e-02],\n",
              "        [ 1.9810e-01, -9.9068e-02, -2.7453e-01,  ...,  1.7955e-01,\n",
              "         -3.1754e-01,  1.3571e-01]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "un9wo5TPyN9L",
        "outputId": "0fd5e0b8-d291-4a6d-eecf-5f6cc5b271d7"
      },
      "source": [
        "\n",
        "feature_1 = vectorized_seqs_1\n",
        "feature_2 = vectorized_seqs_2\n",
        "\n",
        "\n",
        "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
        "#train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\n",
        "train_and_dev = Task1Dataset_double(feature_1,feature_2, train_df['meanGrade'])\n",
        "\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,\n",
        "                                           (train_examples,\n",
        "                                            dev_examples))\n",
        "####Shuffle might need to be true. Check later\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "train_losses, valid_losses = train(train_loader, dev_loader, model, epochs)\n",
        "plot(train_losses, valid_losses, len(train_losses))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataloaders created.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.60 | Train MSE: 0.60 | Train RMSE: 0.77 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
            "| Epoch: 02 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 03 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 04 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 05 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 06 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.60 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 07 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 08 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 09 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.59 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "5mwekgglvjbX",
        "outputId": "623543e9-fc6a-4abe-f8d7-3dd6c8d9ea35"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(train_losses,valid_losses,num_epochs):\n",
        "  epochs = list(range(num_epochs))\n",
        "  plt.plot(epochs,train_losses, label='train')\n",
        "  plt.plot(epochs,valid_losses, label='valid')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1fnH8c8zk40kEBISCBAg7HtYEnYECrIqYBUFFYvYigsKllqr1V9tXVpaa6tSURCorSKIoBaRpVhZZU1ASFhkCVtYw05Ysp7fH3eAgAEmkOTO8rxfr7wyc5eZZ4bwnTPnnnuuGGNQSinluxx2F6CUUqp0adArpZSP06BXSikfp0GvlFI+ToNeKaV8XIDdBVwtOjraxMfH212GUkp5lZSUlKPGmJii1nlc0MfHx5OcnGx3GUop5VVEZM+11mnXjVJK+TgNeqWU8nEa9Eop5eM06JVSysdp0CullI/ToFdKKR+nQa+UUj7OZ4I+J6+AP83dQsaJc3aXopRSHsVngv7w6Qt8snovI6euIzsv3+5ylFLKY/hM0NeICuWv97VgQ8YpXp2z2e5ylFLKY/hM0AP0bhrLY13r8PGqvXy+LsPucpRSyiP4VNAD/LpXQ9rXieK3X6Sy5eBpu8tRSinb+VzQBzgdjLu/NRVCAnni4xROX8i1uySllLKVzwU9QEz5YMY/2JqME+d5dsYG9ALoSil/5pNBD5AUH8UL/Rrz382Hmbg03e5ylFLKNj4b9ACPdIrnjuZV+fP8razceczucpRSyhY+HfQiwp8HJVA7Ooynp63n8OkLdpeklFJlzqeDHiA8OID3hyZyLiePkVPXkZtfYHdJSilVpnw+6AHqVynP2HsSSN5zgrHzttpdjlJKlSm/CHqAAS2q8XDHeCYv38XXGw/aXY5SSpUZvwl6gN/2a0zrmhV5buYGdhzJsrscpZQqE34V9EEBDt59sDUhgU6e+DiFs9l5dpeklFKlzq+CHqBqRDnG3d+KnZlZPP95qp5MpZTyeX4X9AAd60XzbO+GfLXhAP9asdvucpRSqlT5ZdADPN6lLrc3rsJrX28hZc8Ju8tRSqlS47dB73AIb97XgmoVyzFy6jqOZmXbXZJSSpUKvw16gIhygbw3tDUnzuUwatp68gu0v94nZSTDh3fC1PvgsF6URvkfvw56gKbVInj9p81ZsfMYb/73B7vLUSUp6wh8ORIm9YCj22Hfani/E8wZA2d17iPlPwLc2UhE+gBvA05gkjFm7FXrHwdGAvlAFjDCGLNZROKBLcDFBF1ljHm8ZEovOYMS40jZc4Lxi3fSqmYkPZtUsbskdSvyc2HtJFj0R8g9D51GQ5dfW8sXj7XWpc6Ebr+BNo9CQJDdFaviyM+D3LOQcw5yXT85565clnO2iOXXWn8OnIFQpSnEJkDVllA1AcIr2/1KS4zcaHihiDiBbUBPIANYC9xvjNlcaJsKxpjTrtsDgCeNMX1cQT/HGNPM3YKSkpJMcnJycV/HLbuQm8+9769k97GzzHm6M7UqhZV5DaoE7FoKc5+DzC1Qtwf0/TNE179ym8wfYMFvYcc3UKke9HodGvQGEXtq9ldHd8C2+ZB9xo2QLrQ8P6d4z+MIgMAwCAqFwFDX78L3w6zHPpQKJ/dc3i881gr82ITLvyPjPfbvRERSjDFJRa1zp0XfFthhjEl3Pdh0YCBwKegvhrxLGFD2nd3GwLI3odVDUL74LfKQQCfjH2zNneOW8/jH6/jiyY6EBDpLoVBVKk5lwH9fgk1fQMWaMOQTaNiv6P+UMQ1h6CzYvtAK/GmDoW536P1HqNy47Gv3JwUFsGMhrJlofdBeFFCu6AAOj71qebkfb/Oj8L5qvTPQ/frOn7QC/9BGOLjR+r3jf2DyrfUhEVbgFw7/6AbgdKtzxDbutOgHAX2MMb9w3X8IaGeMeeqq7UYCY4AgoLsxZrurRb8J6xvBaeAlY8yyIp5jBDACoGbNmol79uy5epMbO7odJnSBkIowZCpUb138xwAW/XCERz5cyz2t43hjUALioZ/eyiUvG1aMsz7kTQF0HgOdRlmB4I78XFg7GRb/EbKzIGk4dPsthFUq3br9zfkTsH4qrP0ATuy2AjzpEWj9kHXb4cGHC3PPWwfxD224HP6HN0Gea9rzgJBC3T4JENsCqjRx/2+whFyvRV9iQV9o+weA3saYYSISDIQbY46JSCLwJdD0qm8AV7ilrptDqTDtATh7BPq/Ay0G39TD/H3hNt7+33b+dHdz7m9b8+ZqUaVv2wKY/zwcT4fG/a0umMhaN/dY545bffrJUyA4HLq9AG1+UbzWoPqxQ2lW633jDMg7DzU7QNsR1r+XN7+3+XlwbPvl4D+4wfp94ZS1XpxWS/9S108LiG0O5SqWWkm3GvQdgN8bY3q77r8AYIz50zW2dwAnjDERRaxbDDxrjLlmkt9yH/3Zo/DZw7B7GXR8Gm7/AziK1wWTX2AY/uFaVu08xswnOpAQV3r/OOomHE+H+S9Y/bvRDax++LrdS+axj2yxunN2fguV6lvdOfV7emy/rEfKz4Wtc2DNB7DnO6tbJuFe68B31QS7qys9xlh9/JfC3/X7TKHZcivWutzqr+r6ACgfWyJPf6tBH4DV9dID2I91MPYBY8ymQtvUN8Zsd93uD7xsjEkSkRjguDEmX0TqAMuA5saY49d6vhI5GJufa/1nXTPROiA3aDKUiyzWQxw/m0P/ccsB+HpUZyqG6sgM2+WchWV/gxXvgDMIuv4G2j1e8qNmjIHt/7X+ho7tsP6Gev8RKjcq2efxNVlHIOVf1reiMwesYyVtHoVWQyE0yu7q7JN1xBX6hbp+jhe6jnVY5cst/7g20KjfTT3NLQW96wH6AW9hDa+cYox5XUReAZKNMbNF5G3gdiAXOAE8ZYzZJCL3AK+4lhdgfQB8db3nKtFRNyn/gq9/BRVrwP3TrYNwxbBh30nufX8lHetVYsqwNjgc2qqzhTGw+UtY8BKczoCEwdY3tQpVS/d583KsoZhLxlr9921+bnXp+HNoFSUj2WpUbfrCGhFTt7vVPVO/V7G/TfuNC6fhcNqVXT+ZW6F6Evx8wU095C0HfVkq8eGVe1fBpw9ZB1Tu+QAa9i3W7h+v2sNLX6YxpmcDRvWof+MdVMk6sgXmPWcNm6zSHPq9AbU6lG0NZ49ZB2uTp0BwBVf//c+9u4/5VuVesIJ9zUQ4sA6CykPLB6Dtoz8ezqrck5dtdT1HVL+p3f076MEaejf9QetTs/tLcNuv3O5zNcbwq8828MX6/Xw4vC1dG8SUbG2qaBdOWSc3rZ4AweWtf7ekR+xtIR7eDAtegPTF1rGBi/33/uRUhjVKad2/4NwxiG5ohXuLIda/k7KNBj1YLfrZT0PqZ9DkLrhrvDXG1g3nc/L56fjvOHT6AnOe7kxcZGjJ16csBQWwcTos/J3VukkcBt1/5znDHY2xDgIveBGO74R6PaH368XuFvQqxliDG9ZMhK1fW8sa9rMCvnZXPVDtITToLzLGOpC38GWo0swab+/mcLxdR88yYNxy6sSEMePxDgQHaN9jiTuwHub+GjLWWgel+r0B1VrZXVXR8nKs4FvyF8jJskKv6298q/8+Ows2fmqNnsncYg1oaD3M+mZ1s8NYVanRoL/a9m9g5iPW2Wz3/RviO7u124JNh3jsoxSGtq/Ja3c1L90a/cnZY/DtK9bB87Bo6PkKJAzx7JNoLjp7FBa9DikfWmdNdvutddKVN/ffH9tpHYRePxWyT1mjQdo9Bs3uKfOTgJT7NOiLcnQHTL/fGubUZ6x1cowbX0H/NG8LE5ak87f7WnB367jSr9OXFeRbBzi/fc2a76Td49ZEYyE/OgXD8x3eZI3t37XE6rfu80eod7vdVbmvoMCakmDNBOu3I8Dq4mz3mPXtSrtnPJ4G/bVcOAWzHoXtC6yvpP3+esMx2Xn5BQydvJrv953kiyc70bhqhbKp1dfsWQnzfm2dzVy7C/T9i/fPM2MM/DAP/vui1YCo38s6YOvJo1AuTU0wCU7sujw1QeKwEjuRR5UNDfrrKci3vnovexNqtIfBH91wetLMM9nc8c4yQoOczH66MxVCvPhrelk7c8g60LrxU6gQB71fs1qOvtRizMu+3H+fe84aU971uWKftFfiCgouzwJ5OgPW/duamiD3nGtqgkehUX+dttlLadC7I22WdZGK0CgY/PENJ0VL3n2cIRNX0b1RZSY8lKiTn91IXg6sfs8Kv/wc6DgKbhvj9sgnr5SVCYteswI1JAJ+8iIkDr/xTIfGWB8WOWetA705Z11T9J69fPvS8nNXbpOTdTnMr94u9+yVzxMQAs3vtT6IfHlqAj+hQe+ugxth+gNwNhMGjIOE+667+eTlu3h1zmZe6NuIx7rWLaMivdCO/8G831iTQDXoY3VnVPKj9+tQqtV/v3sZxDSyzn68ZoC7wtkUuP/4AeWsD8ygUAgKd90Oc03XW/gn3LVNGARHQL0evjVKyM9p0BfH2aMw42fWZEwdR8Htv7/mSTrGGJ6atp55qQeZ+ov2dKjrIWO9PcWhVOtA67b5EFnbmnysQW+7q7KHMdYY9MV/suY8LyqYfxTO4dfYLvzyXOtBYTrNgAI06IsvP9ea/nbtJGvkxD2Trtm/mpWdx8B/LOfU+Ty+HtWZKhVCyrhYD3R0u3XcY9MXVpdFp2eg/ZMQqO+NUqXlekHvBQOVbeAMhDvehDvfgvQl8EEP6/JzRQgPDuD9oYmcy8lj5NR15OYX4yu3rzmxB758Et5tC9v+C7c9C6M3WH3xGvJK2UaD/nqShsOwryD7tBX2P8wvcrP6Vcoz9p4EkvecYOy8rWVcpAc4cwi+fhbGJVoX3W73hBXwPf7P/pEmSim3rhnr32p1gBGLrYO004Zcc1K0AS2qsW7PCSYv30XrmpHckVDKU+h6gnPHYfnfrVPkC3Ktece7PHfTs+8ppUqHBr07IuJg+Hz4ahR8+6o1j/TAd380NPC3/RqzMeMkz83cQM2oUJrHeeEZnu64cBpWvmv95GRZo5O6PQ9RdeyuTClVBO26cVdQKNz9gTUPy6YvYUpvOLn3yk0CHIx/MJHIsCAenLSKDftO2lRsKck5B8vfgrcTrItx1O0GT66EuydqyCvlwTToi0MEOo2GBz+DE3thYjfY/d0Vm8RGhDB9RHsiQgMZOnk16/eesKfWkpSXDasnwjst4ZuXoXqi1Z01+GPvn7ZAKT+gQX8z6veER/8H5aLg3wOsYZiFxEWG8umIDkSFBfHQ5DWk7LnmJXI9W34erPsIxiVZ89JUqgfD58HQWZ47fbBS6kc06G9WdH0r7Ot2t65L+9Uz1mn+LtUqluPTER2IKR/MzyavYe1uLwr7ggJrSojx7WD2U9ZFP4Z+Dg9/DbU62l2dUqqYNOhvRUiEddHxzmMg5Z9W6z7ryKXVsREhfDqiPVUiQhg2ZQ2r04/ZWKwbLs6+OKGLNV+/I9Dqnnl0kXW6vM7no5RX0qC/VQ4n3P4y3DMZDnwPE39i/XapXMHqs69WsRwP/3MtK3YetbHY60hfDJN7WkNIc7KsA89PfAeN+2vAK+XldAqEknTge+si5FmHoUpTawKryo0gphHHw+rwwIyD7D5xnkk/a0Pn+tF2V2vZtwb+94o14VaF6tZ0ui0f9O4rJCnlh3Sum7KUlQkr3oZDaZC5Fc4cvLTKBJRjh6nG5tyqJLRqT+3GidYHQcVaZT8x1cGN1oRj2xdAWIx1EljicJ2qQCkvdb2g1xOmSlp4DPR67fL98yfh6DY4sgXJ/IH4Q5uouCeNmA3LYYNrm4AQiG5wxTcAYhpBZHzJfwBkboPFf7w84ViP30HbxyA4vGSfRynlMTToS1u5ilCjrfUDBAKB53K474NvMZnb+EMHB00CDsKRLbBnBaTOuLxvQIg1uiemUPhXbnxzHwAn9sCSP8OGadb85V1+DR2esupTSvk07bqxyalzufxsymo2HzzNuw+0pldT1/U5L5y2vgFkbrXCP/MH6/apfZd3dga7vgE0LPQNwPUBcPXVi04fhGV/hZR/gTisy8V1/iWEecgxAqVUidA+eg916nwuw6asIW3/Kf7xQCv6NLvORGjZZ6xul8ytkOn6ADiyFU4VmobBGXT5AyCmkXXx87WToCAPWj1kteJ1wjGlfNItB72I9AHeBpzAJGPM2KvWPw6MBPKBLGCEMWaza90LwM9d60YZYxZc77n8KegBzlzI5eF/ruX7fSd5Z0ir4s96mZ11+RtA5lYr/DO3wsk9gEDCYNeEY7VLpX6llGe4paAXESewDegJZABrgfsvBrlrmwrGmNOu2wOAJ40xfUSkCTANaAtUA74BGhhj8q/1fP4W9GBdpWr4P9ewbu9J/j64JQNaVLv1B805C7nntYtGKT9xq1eYagvsMMakG2NygOnAwMIbXAx5lzDg4qfHQGC6MSbbGLML2OF6PFVIeHAAHw5vS2KtSJ6Zvp4v1++/9QcNCtOQV0oB7gV9daDQkUAyXMuuICIjRWQn8BdgVDH3HSEiySKSnJmZ6W7tPiUsOIAPh7ehXe1KjJnxPbNSMuwuSSnlI0psCgRjzLvGmLrAb4CXirnvRGNMkjEmKSYmpqRK8jqhQQFMebgNHetG8+zMDcxI3nfjnZRS6gbcCfr9QI1C9+Ncy65lOnDXTe7r98oFOZk0LInO9aJ5buZGpq3Ze+OdlFLqOtwJ+rVAfRGpLSJBwBBgduENRKR+obt3ANtdt2cDQ0QkWERqA/WBNbdetm8LCXTywc+S6NYwhhc+T+XjVXvsLkkp5cVueGasMSZPRJ4CFmANr5xijNkkIq8AycaY2cBTInI7kAucAIa59t0kIjOAzUAeMPJ6I27UZSGBTiY8lMiTH6/jpS/TKDCGn3WIt7sspZQX0hOmPFxOXgEjP1nHws2H+d2dTXiks46HV0r92K0Or1Q2Cgpw8O4DrenTNJZX5mxm0rJ0u0tSSnkZDXovEBTgYNwDrbijeVVe+3oL7y/ZaXdJSikvorNXeolAp4O3h7TE4RDGzttKfoFh5E/q2V2WUsoLaNB7kQCng7/f1wKnwBsLfiC/wDCqR/0b76iU8msa9F4mwOngzfuslv3fFm4jv8DwzO31Eb2uq1LqGjTovZDTIbwxqAVOEd7+33YKjGFMzwYa9kqpImnQeymnQ/jzPQk4HcK4b3eQV2B4rndDDXul1I9o0Hsxh0P440+b43QI7y3eSX6B4YW+jTTslVJX0KD3cg6H8NpdzQhwCBOXppOXb/i/Oxtr2CulLtGg9wEiwu8HNMXhEKZ8t4sCY3i5fxMNe6UUoEHvM0SE393ZBKcIk5bvIq+ggFcGNMPh0LBXyt9p0PsQEeHFOxrjdAoTlqSTXwCv36Vhr5S/06D3MSLC830aEeAQ3l1kTZWgYa+Uf9Og90EiwrO9GgJo2CulNOh9lYa9UuoiDXofpmGvlAINep+nYa+U0qD3A1eHvQi8NlDDXil/oUHvJy6GvTEwfrHVstewV8o/aND7ERHh172tlr2GvVL+Q4Pez2jYK+V/NOj9kIa9Uv5Fg95PXR32AryqYa+UT9Kg92NFtew17JXyPRr0fu5i2BvgPQ17pXySBr1CRHjO1bLXsFfK97gV9CLSB3gbcAKTjDFjr1o/BvgFkAdkAo8YY/a41uUDqa5N9xpjBpRQ7aoEadgr5btuGPQi4gTeBXoCGcBaEZltjNlcaLP1QJIx5pyIPAH8BRjsWnfeGNOyhOtWpeDqsBdBL16ilA9wp0XfFthhjEkHEJHpwEDgUtAbYxYV2n4VMLQki1Rlp6iWvYa9Ut7NnaCvDuwrdD8DaHed7X8OzCt0P0REkrG6dcYaY74sdpWqTF0Me2Pg/SUa9kp5uxI9GCsiQ4EkoGuhxbWMMftFpA7wrYikGmN2XrXfCGAEQM2aNUuyJHWTRITf9LFa9hr2Snk3d4J+P1Cj0P0417IriMjtwItAV2NM9sXlxpj9rt/pIrIYaAVcEfTGmInARICkpCRTvJegSktRYf/qwGaIaNgr5U3cCfq1QH0RqY0V8EOABwpvICKtgAlAH2PMkULLI4FzxphsEYkGOmEdqFVeQsNeKe93w6A3xuSJyFPAAqzhlVOMMZtE5BUg2RgzG3gDCAc+cwXAxWGUjYEJIlIAOLD66DcX+UTKY2nYK+Xd3OqjN8bMBeZetex3hW7ffo39VgDNb6VA5Rk07JXyXnpmrHLbxbA3GCYsSQc07JXyBhr0qlhEhOf7NAJgwpJ0BOGVgU017JXyYBr0qtiuDntAw14pD6ZBr26Khr1S3kODXt00DXulvIMGvboll8LewISlGvZKeSINenXLRITn+7pa9kvTEYE/DNCwV8pTaNCrEnF12IOGvVKeQoNelRgNe6U8kwa9KlEa9kp5Hg16VeKuDvvcfMMfBjQlKMBhc2VK+ScNelUqLoa90yGMX7yTzQdP84/7W1EjKtTu0pTyO9rEUqVGRHiuTyPee7A16ZlZ3PHOMuanHbK7LKX8jga9KnV9m1dl7qjbqB0dxuMfp/D72ZvIzsu3uyyl/IYGvSoTNaJC+ezxjjzSqTYfrtjNoPdWsufYWbvLUsovaNCrMhMU4OB3/Zsw8aFE9hw7y53vLOfrjQftLkspn6dBr8pcr6axzB19G/WqhDPyk3W89GUqF3K1K0ep0qJBr2wRFxnKjMc68FiXOny8ai93j1/BrqPalaNUadCgV7YJdDp4oV9jpjycxMFT57nznWX85/v9dpellM/RoFe2696oCnNH30bjqhUYPf17np+1UbtylCpBGvTKI1SNKMf0Ee15sltdpq/dx8B/fMeOI1l2l6WUT9CgVx4jwOnguT6N+NcjbTmalU3/ccuZlZJhd1lKeT0NeuVxujaIYe7o20iIi+BXn23g2c82cC4nz+6ylPJaGvTKI1WpEMLUX7RjVPd6zFqXwcB/fMe2w2fsLkspr6RBrzxWgNPBmF4N+eiRdpw4l8uAfyxnxtp9GGPsLk0pr6JBrzxe5/rRzB3dmcRakTw3ayO//PR7zmZrV45S7tKgV16hcvkQ/v1IO8b0bMDsDQfoP245Ww6etrsspbyCW0EvIn1E5AcR2SEizxexfoyIbBaRjSLyPxGpVWjdMBHZ7voZVpLFK//idAijetRn6i/ak5Wdx8B3v2Pq6j3alaPUDdww6EXECbwL9AWaAPeLSJOrNlsPJBljEoCZwF9c+0YBLwPtgLbAyyISWXLlK3/UoW4l5o6+jXa1o3jxizSenraeMxdy7S5LKY/lTou+LbDDGJNujMkBpgMDC29gjFlkjDnnursKiHPd7g0sNMYcN8acABYCfUqmdOXPosOD+dfwtvy6d0PmpR2i/7jlpO0/ZXdZSnkkd4K+OrCv0P0M17Jr+Tkwrzj7isgIEUkWkeTMzEw3SlIKHA5h5E/qMX1Eey7kFnD3+BX8e+Vu7cpR6iolejBWRIYCScAbxdnPGDPRGJNkjEmKiYkpyZKUH2gTH8Xc0bfRqV4lfvefTTw5dR2nzmtXjlIXuRP0+4Eahe7HuZZdQURuB14EBhhjsouzr1K3KiosiMnD2vBC30Ys3HyYO8ctY8O+k3aXpZRHcCfo1wL1RaS2iAQBQ4DZhTcQkVbABKyQP1Jo1QKgl4hEug7C9nItU6rEORzCY13r8uljHSgogEHvr2Dy8l3alaP83g2D3hiTBzyFFdBbgBnGmE0i8oqIDHBt9gYQDnwmIt+LyGzXvseBV7E+LNYCr7iWKVVqEmtF8vWoznRtUJlX52xmxEcpnDyXY3dZStlGPK21k5SUZJKTk+0uQ/kAYwyTl+/iz/O3Url8CG8PaUlSfJTdZSlVKkQkxRiTVNQ6PTNW+SwR4Re31eGzxzsiAvdOWMlLX6ZyWsfcKz+jQa98XssaFZn/TBce7hjP1NV7uf3NJcxLPah998pvaNArvxAeHMDL/Zvy5ZOdqBQezBNT1/Hov1M4cPK83aUpVeo06JVfaVGjIl891Ynf9mvE8h2Z9PzbEv753S7yC7R1r3yXBr3yOwFOByO61GXhL7uSFB/FH77azN3jv2PzAZ0NU/kmDXrlt2pEhfLh8Da8PaQl+0+ep/8/lvOnuVs4n5Nvd2lKlSgNeuXXRISBLavzzZiuDGodx4Sl6fR6awlLtumcS8p3aNArBVQMDeLPgxL4dER7Ap0Ohk1Zw+jp6zmalX3jnZXycBr0ShXSrk4l5o2+jdE96jMv9RA93lzCp2v36lBM5dU06JW6SnCAk1/2bMDc0Z1pWKU8v5mVypCJq9iZmWV3aUrdFA16pa6hXuXyTB/RnrF3N2fLwdP0fWsZb3+znew8PVirvIsGvVLX4XAIQ9rW5JtfdaV3s1j+/s027nhnOWt369x8ynto0CvlhsrlQxh3fyv+ObwN53Pyuff9lbzweSqnzum8OcrzadArVQw/aViZhWO68Ohttfl07V56/G0JX204oAdrlUfToFeqmEKDAnjxjibMfqozVSNCeHraeh75cC0ZJ87ZXZpSRdKgV+omNasewRdPduT/7mzC6l3H6fm3pUxalk5efoHdpSl1BQ16pW5BgNPBzzvXZuGYrnSsW4nXvt7CXeO/IzXjlN2lKXWJBr1SJaB6xXJMGpbE+Adbc/h0NgPfXc6rczZzNjvP7tKU0qBXqqSICP2aV+WbMV25v21NJi/fRa+/L+XbrYftLk35OQ16pUpYRLlAXv9pc2Y+3oHQICePfJjMyE/WceTMBbtLU35Kg16pUpIUH8XXo27j2V4NWLj5MD3eXMLU1Xso0IucqDImnjb+NykpySQnJ9tdhlIlKj0zixe/SGNl+jFqRJXjntZx3NM6jhpRoXaXpnyEiKQYY5KKXKdBr1TZMMYwN/UQn6zZw4qdxzAGOtSpxKDEOPo2jyU0KMDuEpUX06BXysNknDjH5+v2MzMlg73HzxEeHMAdzasyKCmOpFqRiIjdJSovo0GvlIcyxrBm13FmpmTwdepBzuXkE18plEGJcdzdOo5qFcvZXaLyEhr0SnmBs9l5zE09yMyUDFbvOo4IdK4XzaDEOHo3jSUk0Gl3icqDadAr5WX2HjvHzHUZzErJYM5uOSUAAA/FSURBVP/J85QPCaB/i2oMSoyjVY2K2rWjfuSWg15E+gBvA05gkjFm7FXruwBvAQnAEGPMzELr8oFU1929xpgB13suDXqlLisoMKxKP8bMlAzmph3kQm4BdWPCGJRYg7tbV6dKhRC7S1Qe4paCXkScwDagJ5ABrAXuN8ZsLrRNPFABeBaYfVXQZxljwt0tVoNeqaKduZDL3NSDfJacQfKeEzgEujSIYVBiHLc3rqJdO37uekHvzniutsAOY0y668GmAwOBS0FvjNntWqfT9ilVSsqHBDK4TU0Gt6nJrqNnmZmyj8/X7eepT9YTUS6QAS2qcW9SHM2rR2jXjrqCO0FfHdhX6H4G0K4YzxEiIslAHjDWGPPl1RuIyAhgBEDNmjWL8dBK+afa0WH8uncjxvRsyHc7jjIzJYNPk/fx0ao9NKxSnkGJcdzVqjox5YPtLlV5gLI4Q6OWMWa/iNQBvhWRVGPMzsIbGGMmAhPB6ropg5qU8glOh9ClQQxdGsRw6nwuczYe4LPkDF6fu4Wx87fyk4ZW1073RlUICtAZT/yVO0G/H6hR6H6ca5lbjDH7Xb/TRWQx0ArYed2dlFLFFlEukAfb1eLBdrXYceQMn6Vk8Pm6/Xyz5QiRoYEMbFmde5PiaFotwu5SVRlz52BsANbB2B5YAb8WeMAYs6mIbT8E5lw8GCsikcA5Y0y2iEQDK4GBhQ/kXk0PxipVcvLyC1i2/Sifpezjm81HyMkvoHHVCtybGMcdCVV11I4PKYnhlf2whk86gSnGmNdF5BUg2RgzW0TaAF8AkcAF4JAxpqmIdAQmAAVYM2W+ZYyZfL3nKiroc3NzycjI4MIF/5jmNSQkhLi4OAIDA+0uRfmQE2dzmL3hADNTMkjdfwoRSKoVSd9mVenTLFbPwvVyXn/C1K5duyhfvjyVKlXy+dEExhiOHTvGmTNnqF27tt3lKB+1/fAZ5qYeYl7aQbYeOgNAyxoV6dc8lr7Nquqsml7I64N+y5YtNGrUyOdD/iJjDFu3bqVx48Z2l6L8QHpmFvPSrNBP238agObVI+jbPJZ+zaoSHx1mc4XKHbc6jt4j+EvIg3+9VmW/OjHhjPxJPUb+pB57j51jXtpB5qYd4i/zf+Av83+gcdUK9GsWS9/msdSrXN7uctVN8JqgV0qVvpqVQnmsa10e61qXjBPnmJ92iPlph3hz4TbeXLiN+pXD6du8Kv2ax9KwSnltlHgJHVjrppMnTzJ+/Phi79evXz9OnjxZChUpVbriIkP5xW11mPlER1a90IM/DGhKVFgQ477dTp+3ltHjzSW8sWAraftP4WldwOpKXtNHb3d/9e7du7nzzjtJS0u7YnleXh4BASX/xcgTXrNSRck8k81/Nx9iXuohVqYfI7/AUDMqlL6uA7kt4nQKBjv4RB/9RX/4ahObD5wu0cdsUq0CL/dvet1tnn/+eXbu3EnLli0JDAwkJCSEyMhItm7dyrZt27jrrrvYt28fFy5cYPTo0YwYMQKA+Ph4kpOTycrKom/fvnTu3JkVK1ZQvXp1/vOf/1CunA5pU94lpnzwpROzjp/NYeHmQ8xNPcTkZbuYsCSd6hXL0adZLP2ax9KqRiQOh4a+3bwu6O0yduxY0tLS+P7771m8eDF33HEHaWlpl4ZATpkyhaioKM6fP0+bNm245557qFSp0hWPsX37dqZNm8YHH3zAfffdx6xZsxg6dKgdL0epEhEVFnRporVT53JZuOUw89MO8tHKPUxevosqFYLp26wqfZvFkhQfhVND3xZeF/Q3anmXlbZt214xzv2dd97hiy++AGDfvn1s3779R0Ffu3ZtWrZsCUBiYiK7d+8us3qVKm0RoYEMSoxjUGIcZy7k8u3WI8xNPci0NXv5cMVuosOD6dOsCv2aVaVt7SgCnHqIsKx4XdB7irCwy2OLFy9ezDfffMPKlSsJDQ2lW7duRZ7FGxx8eSZBp9PJ+fPny6RWpcpa+RBrbp2BLatzNjuPRT8cYV7qIWal7OfjVXuJCguiV5Mq9GkWS/s6lXQu/VKmQe+m8uXLc+bMmSLXnTp1isjISEJDQ9m6dSurVq0q4+qU8lxhwQHcmVCNOxOqcT4nnyXbMpmXdpA5Gw8yfe0+QgIdtKtdiS4NYujaIIa6MWF6MLeEadC7qVKlSnTq1IlmzZpRrlw5qlSpcmldnz59eP/992ncuDENGzakffv2NlaqlOcqF+SkT7NY+jSL5UJuPivTj7F0WyZLtmXy6pzNvApUr1iOLg2i6dogho71oqkQonM+3SodXumh/PE1K/+WceIcS7cdZcm2I6zYcYwz2Xk4HUKrGhXp6ppzv3n1CB3Fcw0+NbxSKeWb4iJDeaBdTR5oV5Pc/ALW7z3J0m2ZLN2eeenM3MjQQG6rb4V+l/rRVNZplt2iQa+U8jiBTgdta0fRtnYUz/ZuyLGsbJbvOMqSHzJZuv0oszccAKBx1QqXunmSakXpVbSuQYNeKeXxKoUHXxrFU1Bg2HLoNEu2ZbJ0WyZTllsnaoUGOelQpxJdG8bQpX6MzrpZiAa9UsqrOBxC02oRNK0WwZPd6pGVncfKnZcP6v5v6xEAakaFXurb71C3EuHB/ht3/vvKlVI+ITw4gJ5NqtCziTUSbvfRsyzdnsmSHzKZtS6Dj1btIdApJNaKdPXtx9CkagW/OqirQa+U8inx0WHER4fxsw7xZOflk7L7BEu2Z7J029FLc+xHhwfTpX40XRrEcFv9aCqFB9/4gb2YBn0pCQ8PJysriwMHDjBq1Chmzpz5o226devGX//6V5KSihwRpZS6RcEBTjrWi6ZjvWhe6AtHTl9g6fajLNmWyaIfjvD5+v2IQOPYCrSoUZEWcREkxFWkQZVwn5qiQYO+lFWrVq3IkFdKlb3KFUIuzceTX2BI23+KJdsyWb3rGHM2HmDamr0AhAQ6aFotghZxFWlRwwr/+EqhXnvGrvcF/bzn4VBqyT5mbHPoO/a6mzz//PPUqFGDkSNHAvD73/+egIAAFi1axIkTJ8jNzeW1115j4MCBV+xXeB778+fPM3z4cDZs2ECjRo10rhulbOR0iNWKr1ERqE9BgWH3sbNszDjFhoyTbMw4xSdr9jDluwIAKoQEkBBXkQRXq79FjQhiK4R4Rfh7X9DbZPDgwTzzzDOXgn7GjBksWLCAUaNGUaFCBY4ePUr79u0ZMGDANf/h33vvPUJDQ9myZQsbN26kdevWZfkSlFLX4XAIdWLCqRMTzl2tqgOQl1/AtsNZbMw4yYaMU2zMOMnEpenkFVgzCsSUD77U3dOiRkUSqkcQGRZk58sokvcF/Q1a3qWlVatWHDlyhAMHDpCZmUlkZCSxsbH88pe/ZOnSpTgcDvbv38/hw4eJjY0t8jGWLl3KqFGjAEhISCAhIaEsX4JSqpgCnA6aVKtAk2oVGNLWWnYhN59NB06z0dXq35Bxkm+2HLm0T82oUBLirG6fhLgImlWPIMzmoZ3eF/Q2uvfee5k5cyaHDh1i8ODBTJ06lczMTFJSUggMDCQ+Pr7I6YmVUr4jJNBJYq1IEmtFXlp2+kIuaRmnLrX61+89yZyNBwFwCNSrHG61+l2t/0ZVyxMcUHZTM2vQF8PgwYN59NFHOXr0KEuWLGHGjBlUrlyZwMBAFi1axJ49e667f5cuXfjkk0/o3r07aWlpbNy4sYwqV0qVpgohgZdG91yUeSab1P0n2bDPCv9FW48wMyUDgCCng8ZVy1/q829RoyJ1Y8JL7QpcGvTF0LRpU86cOUP16tWpWrUqDz74IP3796d58+YkJSXRqFGj6+7/xBNPMHz4cBo3bkzjxo1JTEwso8qVUmUtpnww3RtVoXsj60QuYwz7T56/FPwbMk7yxfr9fLTKaiCGBTnp3rgK4+5vVeK16DTFHsofX7NS/qagwJB+NOtS+IcFB/Bcn+s3GK/letMUu3VGgIj0EZEfRGSHiDxfxPouIrJORPJEZNBV64aJyHbXz7CbegVKKeWDHA6hXuXy3JMYxx8GNrvpkL/h89xoAxFxAu8CfYEmwP0i0uSqzfYCDwOfXLVvFPAy0A5oC7wsIpEopZQqM+606NsCO4wx6caYHGA6cMVZQcaY3caYjUDBVfv2BhYaY44bY04AC4E+N1Oop3UxlSZ/eq1KqdLnTtBXB/YVup/hWuYOt/YVkREikiwiyZmZmT96kJCQEI4dO+YXAWiM4dixY4SE6JVzlFIlwyNG3RhjJgITwToYe/X6uLg4MjIyKOpDwBeFhIQQFxdndxlKKR/hTtDvB2oUuh/nWuaO/UC3q/Zd7Oa+lwQGBlK7du3i7qaUUgr3um7WAvVFpLaIBAFDgNluPv4CoJeIRLoOwvZyLVNKKVVGbhj0xpg84CmsgN4CzDDGbBKRV0RkAICItBGRDOBeYIKIbHLtexx4FevDYi3wimuZUkqpMuIVJ0wppZS6vuudMOVxQS8imcD1J425vmjgaAmV4+30vbiSvh9X0vfjMl94L2oZY2KKWuFxQX+rRCT5Wp9q/kbfiyvp+3ElfT8u8/X3wncuiqiUUqpIGvRKKeXjfDHoJ9pdgAfR9+JK+n5cSd+Py3z6vfC5PnqllFJX8sUWvVJKqUI06JVSysf5TNDf6OIo/kREaojIIhHZLCKbRGS03TXZTUScIrJeRObYXYvdRKSiiMwUka0iskVEOthdk51E5Jeu/ydpIjJNRHxu6lifCHo3L47iT/KAXxljmgDtgZF+/n4AjMaawkPB28B8Y0wjoAV+/L6ISHVgFJBkjGkGOLHm8/IpPhH0uHFxFH9ijDlojFnnun0G6z+yu9cQ8DkiEgfcAUyyuxa7iUgE0AWYDGCMyTHGnLS3KtsFAOVEJAAIBQ7YXE+J85Wgv5WLo/g0EYkHWgGr7a3EVm8Bz/HjK6D5o9pAJvBPV1fWJBEJs7souxhj9gN/xboc6kHglDHmv/ZWVfJ8JehVEUQkHJgFPGOMOW13PXYQkTuBI8aYFLtr8RABQGvgPWNMK+As4LfHtFzTpw/E+gCsBoSJyFB7qyp5vhL0t3JxFJ8kIoFYIT/VGPO53fXYqBMwQER2Y3XpdReRj+0tyVYZQIYx5uI3vJlYwe+vbgd2GWMyjTG5wOdAR5trKnG+EvS3cnEUnyMigtUHu8UY8ze767GTMeYFY0ycMSYe6+/iW2OMz7XY3GWMOQTsE5GGrkU9gM02lmS3vUB7EQl1/b/pgQ8enPaIa8beKmNMnohcvDiKE5hijNlkc1l26gQ8BKSKyPeuZb81xsy1sSblOZ4GproaRenAcJvrsY0xZrWIzATWYY1WW48PToegUyAopZSP85WuG6WUUtegQa+UUj5Og14ppXycBr1SSvk4DXqllPJxGvRKKeXjNOiVUsrH/T9pet/qbTvpugAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klgk-IngaRLa"
      },
      "source": [
        "\n",
        "#### Approach 2: No pre-trained representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK9C7EeEaRLc"
      },
      "source": [
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSZbSxUbaRLc"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwwE7oj0aRLd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "ef8abd4c-a186-4a00-cc8d-8dc44958d2af"
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3c7b5eafe872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Baseline for the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBaseline performance:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dev_y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vs5_tGhaRLd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}